{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "98f7f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten, TimeDistributed\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1e5327f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "sequence_length = 30\n",
    "min_sequences_per_class = 10\n",
    "DATA_PATH = '/home/smayan/Desktop/Cricket Pose Estimation /Data'\n",
    "PROCESSED_DATA_PATH = '/home/smayan/Desktop/Cricket Pose Estimation /ProcessedData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe9116ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df11c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e267847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    return np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6988c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_structure():\n",
    "    \"\"\"Check and print data structure\"\"\"\n",
    "    print(\"Checking data structure...\")\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        print(f\"ERROR: {DATA_PATH} does not exist!\")\n",
    "        return False\n",
    "    \n",
    "    actions = [folder for folder in os.listdir(DATA_PATH) \n",
    "               if os.path.isdir(os.path.join(DATA_PATH, folder))]\n",
    "    \n",
    "    if not actions:\n",
    "        print(f\"ERROR: No action folders found in {DATA_PATH}\")\n",
    "        return False\n",
    "    \n",
    "    total_videos = 0\n",
    "    for action in actions:\n",
    "        action_path = os.path.join(DATA_PATH, action)\n",
    "        videos = [f for f in os.listdir(action_path) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "        print(f\"{action}: {len(videos)} videos\")\n",
    "        total_videos += len(videos)\n",
    "    \n",
    "    print(f\"Total videos found: {total_videos}\")\n",
    "    \n",
    "    if total_videos == 0:\n",
    "        print(\"ERROR: No video files found!\")\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a03c8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_processed_data_structure():\n",
    "    \"\"\"Create directory structure for processed data\"\"\"\n",
    "    if not os.path.exists(PROCESSED_DATA_PATH):\n",
    "        os.makedirs(PROCESSED_DATA_PATH)\n",
    "    \n",
    "    sequences_dir = os.path.join(PROCESSED_DATA_PATH, 'sequences')\n",
    "    metadata_dir = os.path.join(PROCESSED_DATA_PATH, 'metadata')\n",
    "    \n",
    "    if not os.path.exists(sequences_dir):\n",
    "        os.makedirs(sequences_dir)\n",
    "    if not os.path.exists(metadata_dir):\n",
    "        os.makedirs(metadata_dir)\n",
    "    \n",
    "    return sequences_dir, metadata_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "157642c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processing_metadata(actions, video_counts, total_sequences, sequences_dir, metadata_dir):\n",
    "    \"\"\"Save metadata about the processing\"\"\"\n",
    "    metadata = {\n",
    "        'processing_date': datetime.now().isoformat(),\n",
    "        'sequence_length': sequence_length,\n",
    "        'min_sequences_per_class': min_sequences_per_class,\n",
    "        'actions': actions.tolist(),\n",
    "        'video_counts': video_counts,\n",
    "        'total_sequences_per_class': total_sequences,\n",
    "        'keypoint_dimensions': 132,  # 33 pose landmarks Ã— 4 values\n",
    "        'data_path': DATA_PATH,\n",
    "        'processed_data_path': PROCESSED_DATA_PATH\n",
    "    }\n",
    "    \n",
    "    # Save as JSON for human readability\n",
    "    metadata_file = os.path.join(metadata_dir, 'processing_metadata.json')\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "    \n",
    "    print(f\"Metadata saved to: {metadata_file}\")\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "26bc0a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sequences_and_labels(sequences, labels, label_map, sequences_dir, metadata_dir):\n",
    "    \"\"\"Save sequences and labels to disk\"\"\"\n",
    "    print(\"\\nSaving processed sequences...\")\n",
    "    \n",
    "    # Save sequences as numpy array\n",
    "    sequences_file = os.path.join(sequences_dir, 'sequences.npy')\n",
    "    np.save(sequences_file, sequences)\n",
    "    print(f\"Sequences saved to: {sequences_file}\")\n",
    "    print(f\"Sequences shape: {sequences.shape}\")\n",
    "    \n",
    "    # Save labels as numpy array\n",
    "    labels_file = os.path.join(sequences_dir, 'labels.npy')\n",
    "    np.save(labels_file, labels)\n",
    "    print(f\"Labels saved to: {labels_file}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    \n",
    "    # Save label mapping\n",
    "    label_map_file = os.path.join(metadata_dir, 'label_map.pkl')\n",
    "    with open(label_map_file, 'wb') as f:\n",
    "        pickle.dump(label_map, f)\n",
    "    print(f\"Label map saved to: {label_map_file}\")\n",
    "    \n",
    "    # Also save label map as JSON for readability\n",
    "    label_map_json = os.path.join(metadata_dir, 'label_map.json')\n",
    "    with open(label_map_json, 'w') as f:\n",
    "        json.dump(label_map, f, indent=4)\n",
    "    \n",
    "    # Calculate and save file sizes\n",
    "    sequences_size = os.path.getsize(sequences_file) / (1024 * 1024)  # MB\n",
    "    labels_size = os.path.getsize(labels_file) / (1024 * 1024)  # MB\n",
    "    \n",
    "    print(f\"\\nFile sizes:\")\n",
    "    print(f\"Sequences: {sequences_size:.2f} MB\")\n",
    "    print(f\"Labels: {labels_size:.2f} MB\")\n",
    "    print(f\"Total: {sequences_size + labels_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "63392bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_sequences():\n",
    "    \"\"\"Load previously processed sequences\"\"\"\n",
    "    sequences_dir = os.path.join(PROCESSED_DATA_PATH, 'sequences')\n",
    "    metadata_dir = os.path.join(PROCESSED_DATA_PATH, 'metadata')\n",
    "    \n",
    "    # Check if processed data exists\n",
    "    sequences_file = os.path.join(sequences_dir, 'sequences.npy')\n",
    "    labels_file = os.path.join(sequences_dir, 'labels.npy')\n",
    "    label_map_file = os.path.join(metadata_dir, 'label_map.pkl')\n",
    "    metadata_file = os.path.join(metadata_dir, 'processing_metadata.json')\n",
    "    \n",
    "    if not all(os.path.exists(f) for f in [sequences_file, labels_file, label_map_file, metadata_file]):\n",
    "        print(\"Processed data not found. Will process videos from scratch.\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    print(\"Loading previously processed sequences...\")\n",
    "    \n",
    "    try:\n",
    "        # Load sequences and labels\n",
    "        sequences = np.load(sequences_file, allow_pickle=True)\n",
    "        labels = np.load(labels_file, allow_pickle=True)\n",
    "        \n",
    "        # Load label map\n",
    "        with open(label_map_file, 'rb') as f:\n",
    "            label_map = pickle.load(f)\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded sequences shape: {sequences.shape}\")\n",
    "        print(f\"Loaded labels shape: {labels.shape}\")\n",
    "        print(f\"Processing date: {metadata['processing_date']}\")\n",
    "        print(f\"Actions: {metadata['actions']}\")\n",
    "        \n",
    "        return sequences, labels, label_map, metadata\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading processed data: {e}\")\n",
    "        print(\"Will process videos from scratch.\")\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "37256e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    actions = np.array(sorted([folder for folder in os.listdir(DATA_PATH) if os.path.isdir(os.path.join(DATA_PATH,folder))]))\n",
    "\n",
    "    print(f\"Detected Cricket Shots:  {actions}\")\n",
    "\n",
    "    for action in actions:\n",
    "        action_path = os.path.join(DATA_PATH, action)\n",
    "        video_files = [f for f in os.listdir(action_path) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "        print(f\"{action}: {len(video_files)} videos\")\n",
    "\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "09e2939c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Cricket Shots:  ['Backfoot punch' 'Cover drive' 'Cut Shot' 'FBD' 'Flick'\n",
      " 'Front Food defence' 'On Drive' 'Pull Shot' 'Reverse Sweep'\n",
      " 'Straight Drive' 'Sweep' 'Uppercut' 'loft']\n",
      "Backfoot punch: 19 videos\n",
      "Cover drive: 29 videos\n",
      "Cut Shot: 43 videos\n",
      "FBD: 15 videos\n",
      "Flick: 22 videos\n",
      "Front Food defence: 32 videos\n",
      "On Drive: 40 videos\n",
      "Pull Shot: 40 videos\n",
      "Reverse Sweep: 30 videos\n",
      "Straight Drive: 25 videos\n",
      "Sweep: 27 videos\n",
      "Uppercut: 29 videos\n",
      "loft: 31 videos\n"
     ]
    }
   ],
   "source": [
    "actions = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7332c052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sequences_from_videos(actions, force_reprocess=False):\n",
    "    \"\"\"Extract sequences from videos with save/load functionality\"\"\"\n",
    "    \n",
    "    # Try to load existing processed data first\n",
    "    if not force_reprocess:\n",
    "        sequences, labels, label_map, metadata = load_processed_sequences()\n",
    "        if sequences is not None:\n",
    "            print(\"Using previously processed sequences.\")\n",
    "            return sequences, labels, label_map\n",
    "    \n",
    "    print(\"Processing videos to extract sequences...\")\n",
    "    \n",
    "    # Create directory structure\n",
    "    sequences_dir, metadata_dir = create_processed_data_structure()\n",
    "    \n",
    "    sequences = []\n",
    "    labels = []\n",
    "    label_map = {label: num for num, label in enumerate(actions)}\n",
    "    video_counts = {}\n",
    "    total_sequences = {}\n",
    "    \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        for action in actions:\n",
    "            print(f\"\\nProcessing {action} videos...\")\n",
    "            action_path = os.path.join(DATA_PATH, action)\n",
    "            \n",
    "            if not os.path.exists(action_path):\n",
    "                print(f\"Warning: Action path {action_path} does not exist. Skipping...\")\n",
    "                continue\n",
    "            \n",
    "            video_files = [f for f in os.listdir(action_path) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "            video_counts[action] = len(video_files)\n",
    "            \n",
    "            if len(video_files) == 0:\n",
    "                print(f\"Warning: No video files found for action {action}\")\n",
    "                continue\n",
    "            \n",
    "            action_sequences = []\n",
    "            \n",
    "            for video_idx, video_file in enumerate(video_files):\n",
    "                print(f\"  Processing video {video_idx + 1}/{len(video_files)}: {video_file}\")\n",
    "                video_path = os.path.join(action_path, video_file)\n",
    "                \n",
    "                try:\n",
    "                    cap = cv.VideoCapture(video_path)\n",
    "                    \n",
    "                    if not cap.isOpened():\n",
    "                        print(f\"    Error: Could not open video {video_file}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Get total frames in video\n",
    "                    total_frames = int(cap.get(cv.CAP_PROP_FRAME_COUNT))\n",
    "                    if total_frames < sequence_length:\n",
    "                        print(f\"    Warning: Video too short ({total_frames} frames), skipping...\")\n",
    "                        cap.release()\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract multiple sequences using sliding window\n",
    "                    stride = max(1, sequence_length // 4)  # 75% overlap\n",
    "                    video_sequences = 0\n",
    "                    \n",
    "                    for start_frame in range(0, total_frames - sequence_length + 1, stride):\n",
    "                        cap.set(cv.CAP_PROP_POS_FRAMES, start_frame)\n",
    "                        sequence = []\n",
    "                        \n",
    "                        for frame_idx in range(sequence_length):\n",
    "                            ret, frame = cap.read()\n",
    "                            if not ret:\n",
    "                                break\n",
    "                            \n",
    "                            # Resize frame for consistency\n",
    "                            frame = cv.resize(frame, (640, 480))\n",
    "                            \n",
    "                            _, results = mediapipe_detection(frame, holistic)\n",
    "                            keypoints = extract_keypoints(results)\n",
    "                            sequence.append(keypoints)\n",
    "                        \n",
    "                        if len(sequence) == sequence_length:\n",
    "                            action_sequences.append(sequence)\n",
    "                            video_sequences += 1\n",
    "                    \n",
    "                    cap.release()\n",
    "                    print(f\"    Extracted {video_sequences} sequences from {video_file}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    Error processing video {video_file}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Data augmentation for classes with fewer sequences\n",
    "            original_count = len(action_sequences)\n",
    "            while len(action_sequences) < min_sequences_per_class and original_count > 0:\n",
    "                # Simple augmentation: add noise\n",
    "                original_seq = np.array(action_sequences[len(action_sequences) % original_count])\n",
    "                noise = np.random.normal(0, 0.01, original_seq.shape)\n",
    "                augmented_seq = original_seq + noise\n",
    "                action_sequences.append(augmented_seq.tolist())\n",
    "            \n",
    "            augmented_count = len(action_sequences) - original_count\n",
    "            if augmented_count > 0:\n",
    "                print(f\"  Added {augmented_count} augmented sequences\")\n",
    "            \n",
    "            # Add sequences and labels\n",
    "            for seq in action_sequences:\n",
    "                sequences.append(seq)\n",
    "                labels.append(label_map[action])\n",
    "            \n",
    "            total_sequences[action] = len(action_sequences)\n",
    "            print(f\"  Total sequences for {action}: {len(action_sequences)}\")\n",
    "    \n",
    "    # Check if we have any sequences\n",
    "    if len(sequences) == 0:\n",
    "        raise ValueError(\"No sequences were extracted! Check your video files and ensure they are readable.\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    sequences = np.stack(sequences)  # (num_samples, sequence_length, num_features)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    print(f\"\\nFinal dataset:\")\n",
    "    print(f\"Total sequences: {len(sequences)}\")\n",
    "    print(f\"Sequence shape: {sequences.shape}\")\n",
    "    \n",
    "    # Save processed data\n",
    "    save_sequences_and_labels(sequences, labels, label_map, sequences_dir, metadata_dir)\n",
    "    save_processing_metadata(actions, video_counts, total_sequences, sequences_dir, metadata_dir)\n",
    "    \n",
    "    return sequences, labels, label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "93c93ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cricket_data():\n",
    "    \"\"\"Load cricket shot data with variable number of videos per class\"\"\"\n",
    "    if not check_data_structure():\n",
    "        raise FileNotFoundError(f\"Data validation failed for {DATA_PATH}\")\n",
    "    \n",
    "    actions = np.array(sorted([folder for folder in os.listdir(DATA_PATH) \n",
    "                              if os.path.isdir(os.path.join(DATA_PATH, folder))]))\n",
    "    \n",
    "    if len(actions) == 0:\n",
    "        raise ValueError(f\"No action folders found in {DATA_PATH}\")\n",
    "    \n",
    "    print(f\"Detected cricket shots: {actions}\")\n",
    "    \n",
    "    # Count videos per class\n",
    "    for action in actions:\n",
    "        action_path = os.path.join(DATA_PATH, action)\n",
    "        video_files = [f for f in os.listdir(action_path) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "        print(f\"{action}: {len(video_files)} videos\")\n",
    "    \n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "947d88a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hybrid_cnn_lstm_model(input_shape, num_classes):\n",
    "    \"\"\"Create a Hybrid CNN-LSTM model for cricket pose estimation\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # CNN layers for spatial feature extraction\n",
    "    model.add(TimeDistributed(Conv1D(64, kernel_size=3, activation='relu'), \n",
    "                             input_shape=input_shape))\n",
    "    model.add(TimeDistributed(Conv1D(64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv1D(128, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Conv1D(128, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    # Flatten the CNN output for LSTM\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    \n",
    "    # LSTM layers for temporal sequence modeling\n",
    "    model.add(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))\n",
    "    model.add(LSTM(64, return_sequences=False, dropout=0.3, recurrent_dropout=0.3))\n",
    "    \n",
    "    # Dense layers for classification\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1926edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cricket_model(force_reprocess=False):\n",
    "    \"\"\"Main training pipeline with save/load functionality\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        actions = load_cricket_data()\n",
    "        \n",
    "        # Extract or load sequences\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"SEQUENCE EXTRACTION/LOADING\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        X, y, label_map = extract_sequences_from_videos(actions, force_reprocess=force_reprocess)\n",
    "        \n",
    "        # DEBUG INFO\n",
    "        print(f\"\\nDEBUG INFO:\")\n",
    "        print(f\"X type: {type(X)}\")\n",
    "        print(f\"X shape: {X.shape}\")\n",
    "        print(f\"X ndim: {X.ndim}\")\n",
    "        print(f\"y type: {type(y)}\")\n",
    "        print(f\"y shape: {y.shape}\")\n",
    "        \n",
    "        # Validation checks\n",
    "        if X.size == 0:\n",
    "            raise ValueError(\"No sequences were extracted! Check your video files and data path.\")\n",
    "        \n",
    "        if X.ndim != 3:\n",
    "            raise ValueError(f\"Expected X to have 3 dimensions (samples, timesteps, features), got {X.ndim} dimensions with shape {X.shape}\")\n",
    "        \n",
    "        if len(X) != len(y):\n",
    "            raise ValueError(f\"Mismatch between sequences ({len(X)}) and labels ({len(y)})\")\n",
    "        \n",
    "        print(f\"\\nDataset summary:\")\n",
    "        print(f\"Dataset shape: {X.shape}\")\n",
    "        print(f\"Labels shape: {y.shape}\")\n",
    "        print(f\"Classes: {list(label_map.keys())}\")\n",
    "        \n",
    "        # Print class distribution\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        print(f\"\\nClass distribution:\")\n",
    "        for class_idx, count in zip(unique, counts):\n",
    "            class_name = [k for k, v in label_map.items() if v == class_idx][0]\n",
    "            print(f\"  {class_name}: {count} sequences\")\n",
    "        \n",
    "        # Check if we have enough data for training\n",
    "        if len(unique) < 2:\n",
    "            raise ValueError(\"Need at least 2 classes for training\")\n",
    "        \n",
    "        min_class_size = min(counts)\n",
    "        if min_class_size < 2:\n",
    "            raise ValueError(f\"Need at least 2 samples per class. Found class with only {min_class_size} samples\")\n",
    "        \n",
    "        # Reshape X for CNN input (samples, timesteps, features, 1)\n",
    "        X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "        print(f\"Reshaped X to: {X.shape}\")\n",
    "        \n",
    "        # Convert labels to categorical\n",
    "        y_categorical = to_categorical(y, num_classes=len(actions))\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y_categorical, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTraining set: {X_train.shape}\")\n",
    "        print(f\"Test set: {X_test.shape}\")\n",
    "        \n",
    "        # Handle class imbalance\n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced', \n",
    "            classes=np.unique(y), \n",
    "            y=y\n",
    "        )\n",
    "        class_weight_dict = dict(enumerate(class_weights))\n",
    "        \n",
    "        print(f\"\\nClass weights: {class_weight_dict}\")\n",
    "        \n",
    "        # Create model\n",
    "        model = create_hybrid_cnn_lstm_model(\n",
    "            input_shape=(sequence_length, X.shape[2], 1), \n",
    "            num_classes=len(actions)\n",
    "        )\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel summary:\")\n",
    "        model.summary()\n",
    "        \n",
    "        # Create timestamp for TensorBoard logs\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        log_dir = f'logs/cricket_model_{timestamp}'\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            TensorBoard(\n",
    "                log_dir=log_dir,\n",
    "                histogram_freq=1,\n",
    "                write_graph=True,\n",
    "                update_freq='epoch'\n",
    "            ),\n",
    "            EarlyStopping(patience=15, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-7)\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nTensorBoard logs will be saved to: {log_dir}\")\n",
    "        print(f\"Run 'tensorboard --logdir={log_dir}' or use '%tensorboard --logdir {log_dir}' in Jupyter\")\n",
    "        \n",
    "        # Train model\n",
    "        print(f\"\\nStarting training...\")\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=100,\n",
    "            batch_size=16,\n",
    "            validation_data=(X_test, y_test),\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
    "        \n",
    "        # Save model  \n",
    "        model.save('cricket_pose_model.h5')\n",
    "        model.save('cricket_pose_model.keras')\n",
    "        \n",
    "        # Save label mapping for inference\n",
    "        np.save('cricket_label_map.npy', label_map)\n",
    "        \n",
    "        print(f\"\\nModel and label map saved!\")\n",
    "        print(f\"TensorBoard logs saved to: {log_dir}\")\n",
    "        \n",
    "        return model, history, label_map\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in training pipeline: {e}\")\n",
    "        print(\"Please check your data and try again.\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c8710dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def real_time_cricket_prediction():\n",
    "#     \"\"\"Real-time cricket shot prediction\"\"\"\n",
    "#     model = tf.keras.models.load_model('cricket_pose_model.h5')\n",
    "#     label_map = np.load('cricket_label_map.npy', allow_pickle=True).item()\n",
    "#     actions = list(label_map.keys())\n",
    "    \n",
    "#     # Prediction variables\n",
    "#     sequence = []\n",
    "#     predictions = []\n",
    "#     threshold = 0.7\n",
    "    \n",
    "#     cap = cv.VideoCapture(0) \n",
    "    \n",
    "#     with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "#         while cap.isOpened():\n",
    "#             ret, frame = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "            \n",
    "#             # Make detections\n",
    "#             image, results = mediapipe_detection(frame, holistic)\n",
    "#             draw_styled_landmarks(image, results)\n",
    "            \n",
    "#             # Extract keypoints\n",
    "#             keypoints = extract_keypoints(results)\n",
    "#             sequence.append(keypoints)\n",
    "#             sequence = sequence[-sequence_length:]\n",
    "            \n",
    "#             if len(sequence) == sequence_length:\n",
    "#                 # Reshape for model input\n",
    "#                 input_seq = np.expand_dims(np.array(sequence), axis=0)\n",
    "#                 input_seq = input_seq.reshape(1, sequence_length, -1, 1)\n",
    "                \n",
    "#                 # Make prediction\n",
    "#                 res = model.predict(input_seq, verbose=0)[0]\n",
    "#                 predicted_action = actions[np.argmax(res)]\n",
    "#                 confidence = np.max(res)\n",
    "                \n",
    "#                 # Display prediction\n",
    "#                 if confidence > threshold:\n",
    "#                     cv.putText(image, f'{predicted_action}: {confidence:.2f}', \n",
    "#                               (10, 50), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                \n",
    "#                 # Visualization of probabilities\n",
    "#                 for i, (action, prob) in enumerate(zip(actions, res)):\n",
    "#                     y_pos = 100 + i * 30\n",
    "#                     cv.rectangle(image, (10, y_pos), (int(prob * 300) + 10, y_pos + 25), \n",
    "#                                (0, 255, 0), -1)\n",
    "#                     cv.putText(image, f'{action}: {prob:.2f}', (15, y_pos + 18), \n",
    "#                               cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "            \n",
    "#             cv.imshow('Cricket Pose Estimation', image)\n",
    "            \n",
    "#             if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "#                 break\n",
    "    \n",
    "#     cap.release()\n",
    "#     cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "01913cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cricket Pose Estimation with Enhanced Debug and TensorBoard\n",
      "============================================================\n",
      "Checking data structure...\n",
      "Sweep: 27 videos\n",
      "FBD: 15 videos\n",
      "Cover drive: 29 videos\n",
      "Reverse Sweep: 30 videos\n",
      "loft: 31 videos\n",
      "Straight Drive: 25 videos\n",
      "Backfoot punch: 19 videos\n",
      "Cut Shot: 43 videos\n",
      "Flick: 22 videos\n",
      "Front Food defence: 32 videos\n",
      "On Drive: 40 videos\n",
      "Uppercut: 29 videos\n",
      "Pull Shot: 40 videos\n",
      "Total videos found: 382\n",
      "\n",
      "Options:\n",
      "1. Train model (use existing processed sequences if available)\n",
      "2. Train model (force reprocess all videos)\n",
      "3. Clear processed data\n",
      "4. Real-time prediction\n",
      "Checking data structure...\n",
      "Sweep: 27 videos\n",
      "FBD: 15 videos\n",
      "Cover drive: 29 videos\n",
      "Reverse Sweep: 30 videos\n",
      "loft: 31 videos\n",
      "Straight Drive: 25 videos\n",
      "Backfoot punch: 19 videos\n",
      "Cut Shot: 43 videos\n",
      "Flick: 22 videos\n",
      "Front Food defence: 32 videos\n",
      "On Drive: 40 videos\n",
      "Uppercut: 29 videos\n",
      "Pull Shot: 40 videos\n",
      "Total videos found: 382\n",
      "Detected cricket shots: ['Backfoot punch' 'Cover drive' 'Cut Shot' 'FBD' 'Flick'\n",
      " 'Front Food defence' 'On Drive' 'Pull Shot' 'Reverse Sweep'\n",
      " 'Straight Drive' 'Sweep' 'Uppercut' 'loft']\n",
      "Backfoot punch: 19 videos\n",
      "Cover drive: 29 videos\n",
      "Cut Shot: 43 videos\n",
      "FBD: 15 videos\n",
      "Flick: 22 videos\n",
      "Front Food defence: 32 videos\n",
      "On Drive: 40 videos\n",
      "Pull Shot: 40 videos\n",
      "Reverse Sweep: 30 videos\n",
      "Straight Drive: 25 videos\n",
      "Sweep: 27 videos\n",
      "Uppercut: 29 videos\n",
      "loft: 31 videos\n",
      "\n",
      "==================================================\n",
      "SEQUENCE EXTRACTION/LOADING\n",
      "==================================================\n",
      "Processing videos to extract sequences...\n",
      "\n",
      "Processing Backfoot punch videos...\n",
      "  Processing video 1/19: VID_20250723_115632159_00001220.mov\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753627614.350877    7643 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1753627614.406537   12658 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 570.172.08), renderer: NVIDIA GeForce RTX 4070 SUPER/PCIe/SSE2\n",
      "W0000 00:00:1753627614.438845   12631 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753627614.457904   12645 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753627614.458884   12640 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753627614.459289   12634 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753627614.459500   12636 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753627614.462163   12655 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753627614.468851   12649 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753627614.468941   12657 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted 6 sequences from VID_20250723_115632159_00001220.mov\n",
      "  Processing video 2/19: VID_20250723_115632159_00002115.mov\n",
      "    Extracted 5 sequences from VID_20250723_115632159_00002115.mov\n",
      "  Processing video 3/19: VID_20250723_115632159_00000905.mov\n",
      "    Extracted 6 sequences from VID_20250723_115632159_00000905.mov\n",
      "  Processing video 4/19: VID_20250723_115632159_00000735.mov\n",
      "    Extracted 6 sequences from VID_20250723_115632159_00000735.mov\n",
      "  Processing video 5/19: VID_20250723_115632159_00002435.mov\n",
      "    Extracted 6 sequences from VID_20250723_115632159_00002435.mov\n",
      "  Processing video 6/19: VID_20250723_115632159_00002600.mov\n",
      "    Extracted 6 sequences from VID_20250723_115632159_00002600.mov\n",
      "  Processing video 7/19: VID_20250723_115632159_00000555.mov\n",
      "    Extracted 5 sequences from VID_20250723_115632159_00000555.mov\n",
      "  Processing video 8/19: VID_20250723_115632159_00000125.mov\n",
      "    Extracted 6 sequences from VID_20250723_115632159_00000125.mov\n",
      "  Processing video 9/19: VID_20250723_115632159_00001050.mov\n",
      "    Extracted 8 sequences from VID_20250723_115632159_00001050.mov\n",
      "  Processing video 10/19: VID_20250723_115632159_00002920.mov\n",
      "    Extracted 5 sequences from VID_20250723_115632159_00002920.mov\n",
      "  Processing video 11/19: VID_20250723_115632159_00001575.mov\n",
      "    Extracted 6 sequences from VID_20250723_115632159_00001575.mov\n",
      "  Processing video 12/19: VID_20250723_115632159_00001775.mov\n",
      "    Extracted 6 sequences from VID_20250723_115632159_00001775.mov\n",
      "  Processing video 13/19: VID_20250723_115632159_00003050.mov\n",
      "    Extracted 5 sequences from VID_20250723_115632159_00003050.mov\n",
      "  Processing video 14/19: VID_20250723_115632159_00001385.mov\n",
      "    Extracted 7 sequences from VID_20250723_115632159_00001385.mov\n",
      "  Processing video 15/19: VID_20250723_115632159_00000275.mov\n",
      "    Extracted 7 sequences from VID_20250723_115632159_00000275.mov\n",
      "  Processing video 16/19: VID_20250723_115632159_00002770.mov\n",
      "    Extracted 5 sequences from VID_20250723_115632159_00002770.mov\n",
      "  Processing video 17/19: VID_20250723_115632159_00000430.mov\n",
      "    Extracted 5 sequences from VID_20250723_115632159_00000430.mov\n",
      "  Processing video 18/19: VID_20250723_115632159_00001945.mov\n",
      "    Extracted 8 sequences from VID_20250723_115632159_00001945.mov\n",
      "  Processing video 19/19: VID_20250723_115632159_00002255.mov\n",
      "    Extracted 4 sequences from VID_20250723_115632159_00002255.mov\n",
      "  Total sequences for Backfoot punch: 112\n",
      "\n",
      "Processing Cover drive videos...\n",
      "  Processing video 1/29: CD00000575.mov\n",
      "    Extracted 6 sequences from CD00000575.mov\n",
      "  Processing video 2/29: CD00003525.mov\n",
      "    Extracted 6 sequences from CD00003525.mov\n",
      "  Processing video 3/29: CD00000175.mov\n",
      "    Extracted 4 sequences from CD00000175.mov\n",
      "  Processing video 4/29: CD00000885.mov\n",
      "    Extracted 5 sequences from CD00000885.mov\n",
      "  Processing video 5/29: CD00004710.mov\n",
      "    Extracted 4 sequences from CD00004710.mov\n",
      "  Processing video 6/29: CD00000395.mov\n",
      "    Extracted 7 sequences from CD00000395.mov\n",
      "  Processing video 7/29: CD00004870.mov\n",
      "    Extracted 3 sequences from CD00004870.mov\n",
      "  Processing video 8/29: CD00005030.mov\n",
      "    Extracted 4 sequences from CD00005030.mov\n",
      "  Processing video 9/29: CD00001715.mov\n",
      "    Extracted 6 sequences from CD00001715.mov\n",
      "  Processing video 10/29: CD00003315.mov\n",
      "    Extracted 6 sequences from CD00003315.mov\n",
      "  Processing video 11/29: CD00005500.mov\n",
      "    Extracted 2 sequences from CD00005500.mov\n",
      "  Processing video 12/29: CD00005330.mov\n",
      "    Extracted 3 sequences from CD00005330.mov\n",
      "  Processing video 13/29: CD00001910.mov\n",
      "    Extracted 5 sequences from CD00001910.mov\n",
      "  Processing video 14/29: CD00003970.mov\n",
      "    Extracted 5 sequences from CD00003970.mov\n",
      "  Processing video 15/29: CD00002510.mov\n",
      "    Extracted 5 sequences from CD00002510.mov\n",
      "  Processing video 16/29: CD00004230.mov\n",
      "    Extracted 5 sequences from CD00004230.mov\n",
      "  Processing video 17/29: CD00001510.mov\n",
      "    Extracted 6 sequences from CD00001510.mov\n",
      "  Processing video 18/29: CD00001360.mov\n",
      "    Extracted 6 sequences from CD00001360.mov\n",
      "  Processing video 19/29: CD00000275.mov\n",
      "    Extracted 4 sequences from CD00000275.mov\n",
      "  Processing video 20/29: CD00002920.mov\n",
      "    Extracted 8 sequences from CD00002920.mov\n",
      "  Processing video 21/29: CD00002710.mov\n",
      "    Extracted 7 sequences from CD00002710.mov\n",
      "  Processing video 22/29: CD00002320.mov\n",
      "    Extracted 6 sequences from CD00002320.mov\n",
      "  Processing video 23/29: CD00003705.mov\n",
      "    Extracted 6 sequences from CD00003705.mov\n",
      "  Processing video 24/29: CD00001215.mov\n",
      "    Extracted 5 sequences from CD00001215.mov\n",
      "  Processing video 25/29: CD00001035.mov\n",
      "    Extracted 8 sequences from CD00001035.mov\n",
      "  Processing video 26/29: CD00000735.mov\n",
      "    Extracted 5 sequences from CD00000735.mov\n",
      "  Processing video 27/29: CD00005180.mov\n",
      "    Extracted 4 sequences from CD00005180.mov\n",
      "  Processing video 28/29: CD00004440.mov\n",
      "    Extracted 5 sequences from CD00004440.mov\n",
      "  Processing video 29/29: CD00002110.mov\n",
      "    Extracted 6 sequences from CD00002110.mov\n",
      "  Total sequences for Cover drive: 152\n",
      "\n",
      "Processing Cut Shot videos...\n",
      "  Processing video 1/43: cut shot00002355.mov\n",
      "    Extracted 3 sequences from cut shot00002355.mov\n",
      "  Processing video 2/43: cut shot00004900.mov\n",
      "    Extracted 4 sequences from cut shot00004900.mov\n",
      "  Processing video 3/43: cut shot00001590.mov\n",
      "    Extracted 3 sequences from cut shot00001590.mov\n",
      "  Processing video 4/43: cut shot00005005.mov\n",
      "    Extracted 3 sequences from cut shot00005005.mov\n",
      "  Processing video 5/43: cut shot00004245.mov\n",
      "    Extracted 6 sequences from cut shot00004245.mov\n",
      "  Processing video 6/43: cut shot00004645.mov\n",
      "    Extracted 6 sequences from cut shot00004645.mov\n",
      "  Processing video 7/43: cut shot00002060.mov\n",
      "    Extracted 4 sequences from cut shot00002060.mov\n",
      "  Processing video 8/43: cut shot00002880.mov\n",
      "    Extracted 4 sequences from cut shot00002880.mov\n",
      "  Processing video 9/43: cut shot00004390.mov\n",
      "    Extracted 5 sequences from cut shot00004390.mov\n",
      "  Processing video 10/43: cut shot00002620.mov\n",
      "    Extracted 4 sequences from cut shot00002620.mov\n",
      "  Processing video 11/43: cut shot00003275.mov\n",
      "    Extracted 5 sequences from cut shot00003275.mov\n",
      "  Processing video 12/43: cut shot00001225.mov\n",
      "    Extracted 3 sequences from cut shot00001225.mov\n",
      "  Processing video 13/43: cut shot00005240.mov\n",
      "    Extracted 8 sequences from cut shot00005240.mov\n",
      "  Processing video 14/43: cut shot00004760.mov\n",
      "    Extracted 4 sequences from cut shot00004760.mov\n",
      "  Processing video 15/43: cut shot00004515.mov\n",
      "    Extracted 5 sequences from cut shot00004515.mov\n",
      "  Processing video 16/43: cut shot00003145.mov\n",
      "    Extracted 5 sequences from cut shot00003145.mov\n",
      "  Processing video 17/43: cut shot00003775.mov\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m     model, history, label_map \u001b[38;5;241m=\u001b[39m train_cricket_model(force_reprocess\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 32\u001b[0m     model, history, label_map \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_cricket_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforce_reprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     34\u001b[0m     clear_processed_data()\n",
      "Cell \u001b[0;32mIn[61], line 13\u001b[0m, in \u001b[0;36mtrain_cricket_model\u001b[0;34m(force_reprocess)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSEQUENCE EXTRACTION/LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m X, y, label_map \u001b[38;5;241m=\u001b[39m \u001b[43mextract_sequences_from_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_reprocess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# DEBUG INFO\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDEBUG INFO:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[58], line 74\u001b[0m, in \u001b[0;36mextract_sequences_from_videos\u001b[0;34m(actions, force_reprocess)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Resize frame for consistency\u001b[39;00m\n\u001b[1;32m     72\u001b[0m frame \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mresize(frame, (\u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m480\u001b[39m))\n\u001b[0;32m---> 74\u001b[0m _, results \u001b[38;5;241m=\u001b[39m \u001b[43mmediapipe_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mholistic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m extract_keypoints(results)\n\u001b[1;32m     76\u001b[0m sequence\u001b[38;5;241m.\u001b[39mappend(keypoints)\n",
      "Cell \u001b[0;32mIn[48], line 4\u001b[0m, in \u001b[0;36mmediapipe_detection\u001b[0;34m(image, model)\u001b[0m\n\u001b[1;32m      2\u001b[0m image \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mcvtColor(image, cv\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m      3\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      6\u001b[0m image \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mcvtColor(image, cv\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR)\n",
      "File \u001b[0;32m~/Desktop/AI-ML-DS/AI-and-ML-Course/.conda/lib/python3.11/site-packages/mediapipe/python/solutions/holistic.py:160\u001b[0m, in \u001b[0;36mHolistic.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[1;32m    137\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks, left and right hand landmarks, and face landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AI-ML-DS/AI-and-ML-Course/.conda/lib/python3.11/site-packages/mediapipe/python/solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[1;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def clear_processed_data():\n",
    "    \"\"\"Clear all processed data to force reprocessing\"\"\"\n",
    "    import shutil\n",
    "    if os.path.exists(PROCESSED_DATA_PATH):\n",
    "        shutil.rmtree(PROCESSED_DATA_PATH)\n",
    "        print(f\"Cleared processed data directory: {PROCESSED_DATA_PATH}\")\n",
    "    else:\n",
    "        print(\"No processed data directory found to clear.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Cricket Pose Estimation with Enhanced Debug and TensorBoard\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check data structure first\n",
    "    if not check_data_structure():\n",
    "        print(\"Please fix the data structure issues and try again.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Options for running\n",
    "    print(\"\\nOptions:\")\n",
    "    print(\"1. Train model (use existing processed sequences if available)\")\n",
    "    print(\"2. Train model (force reprocess all videos)\")\n",
    "    print(\"3. Clear processed data\")\n",
    "    print(\"4. Real-time prediction\")\n",
    "    \n",
    "    choice = input(\"\\nEnter your choice (1-4): \").strip()\n",
    "    \n",
    "    try:\n",
    "        if choice == '1':\n",
    "            model, history, label_map = train_cricket_model(force_reprocess=False)\n",
    "        elif choice == '2':\n",
    "            model, history, label_map = train_cricket_model(force_reprocess=True)\n",
    "        elif choice == '3':\n",
    "            clear_processed_data()\n",
    "        elif choice == '4':\n",
    "            real_time_cricket_prediction()\n",
    "        else:\n",
    "            print(\"Invalid choice. Running default training...\")\n",
    "            model, history, label_map = train_cricket_model(force_reprocess=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Program terminated with error: {e}\")\n",
    "        print(\"Please check the error messages above and fix any issues.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da198c61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
