{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca3a591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 15:26:23.108782: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-28 15:26:23.115680: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753696583.123665  375958 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753696583.126073  375958 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753696583.132361  375958 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753696583.132372  375958 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753696583.132373  375958 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753696583.132373  375958 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-28 15:26:23.134660: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten, TimeDistributed, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07edd24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e17d6f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/smayan/Desktop/Cricket Pose Estimation /Data'\n",
    "sequence_length = 30\n",
    "min_sequences_per_class = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a35a889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected cricket shots: ['Backfoot punch' 'Cover drive' 'Cut Shot' 'FBD' 'Flick'\n",
      " 'Front Foot defence' 'On Drive' 'Pull Shot' 'Reverse Sweep' 'Stance'\n",
      " 'Straight Drive' 'Sweep' 'Uppercut' 'loft']\n",
      "Backfoot punch: 19 videos\n",
      "Cover drive: 25 videos\n",
      "Cut Shot: 25 videos\n",
      "FBD: 15 videos\n",
      "Flick: 22 videos\n",
      "Front Foot defence: 25 videos\n",
      "On Drive: 25 videos\n",
      "Pull Shot: 25 videos\n",
      "Reverse Sweep: 25 videos\n",
      "Stance: 25 videos\n",
      "Straight Drive: 25 videos\n",
      "Sweep: 25 videos\n",
      "Uppercut: 25 videos\n",
      "loft: 25 videos\n"
     ]
    }
   ],
   "source": [
    "actions = np.array(sorted([folder for folder in os.listdir(DATA_PATH) \n",
    "                          if os.path.isdir(os.path.join(DATA_PATH, folder))]))\n",
    "print(f\"Detected cricket shots: {actions}\")\n",
    "for action in actions:\n",
    "    video_files = [f for f in os.listdir(os.path.join(DATA_PATH, action)) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "    video_files = video_files[:25]\n",
    "    print(f\"{action}: {len(video_files)} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6706a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "labels = []\n",
    "label_map = {label: num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c8c1687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1827, 30, 132)\n",
      "Labels shape: (1827,)\n"
     ]
    }
   ],
   "source": [
    "X = np.load('training_data_noresize.npy')\n",
    "y = np.load('labels.npy')\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "769bc186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.5554406 ,  0.24139841, -0.18845586, ...,  0.77534622,\n",
       "          0.08676209,  0.38804743],\n",
       "        [ 0.55521965,  0.24002098, -0.25809762, ...,  0.7693364 ,\n",
       "          0.21733756,  0.39571512],\n",
       "        [ 0.55163604,  0.23883222, -0.26282305, ...,  0.76868081,\n",
       "          0.25477239,  0.40266779],\n",
       "        ...,\n",
       "        [ 0.37919977,  0.29017907,  0.15896487, ...,  0.72024399,\n",
       "          0.3753553 ,  0.7028569 ],\n",
       "        [ 0.37274122,  0.29221627,  0.21446192, ...,  0.72471052,\n",
       "          0.29324484,  0.72602713],\n",
       "        [ 0.36856019,  0.29323617,  0.14072758, ...,  0.72836959,\n",
       "          0.10429919,  0.74803698]],\n",
       "\n",
       "       [[ 0.50944591,  0.25721657, -0.21083091, ...,  0.76363158,\n",
       "          0.1139462 ,  0.71500087],\n",
       "        [ 0.52320641,  0.25505579, -0.24155426, ...,  0.77121109,\n",
       "          0.11704806,  0.68001729],\n",
       "        [ 0.51852798,  0.25573382, -0.22674216, ...,  0.77223337,\n",
       "          0.14097594,  0.6512875 ],\n",
       "        ...,\n",
       "        [ 0.34827006,  0.29428247,  0.15070571, ...,  0.73209471,\n",
       "         -0.26561132,  0.84881961],\n",
       "        [ 0.34717399,  0.29403761,  0.08361106, ...,  0.73071015,\n",
       "         -0.2880258 ,  0.84440207],\n",
       "        [ 0.34247047,  0.27182987,  0.17902488, ...,  0.72743881,\n",
       "         -0.25761878,  0.84032238]],\n",
       "\n",
       "       [[ 0.46119088,  0.26775602, -0.34944972, ...,  0.76438141,\n",
       "          0.09549927,  0.79977578],\n",
       "        [ 0.46975768,  0.26732156, -0.28787938, ...,  0.76601988,\n",
       "          0.18426935,  0.75819725],\n",
       "        [ 0.46042785,  0.26795703, -0.11246568, ...,  0.75917876,\n",
       "          0.22092137,  0.74011374],\n",
       "        ...,\n",
       "        [ 0.34906569,  0.2644906 ,  0.03881794, ...,  0.72469103,\n",
       "         -0.14383022,  0.85295111],\n",
       "        [ 0.34868261,  0.2656492 ,  0.00229136, ...,  0.72478062,\n",
       "         -0.09825981,  0.8487196 ],\n",
       "        [ 0.34778631,  0.26581252, -0.0065093 , ...,  0.72446036,\n",
       "         -0.09571968,  0.84646028]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.41325703,  0.1890786 , -0.1808124 , ...,  0.84397739,\n",
       "          0.3274419 ,  0.74332148],\n",
       "        [ 0.41286901,  0.20064361, -0.17787069, ...,  0.84680694,\n",
       "          0.22240485,  0.74955237],\n",
       "        [ 0.41232276,  0.1985914 , -0.15576217, ...,  0.8462159 ,\n",
       "          0.14289977,  0.75005841],\n",
       "        ...,\n",
       "        [ 0.43202677,  0.10250124, -0.171857  , ...,  0.87657064,\n",
       "         -0.16311722,  0.74794465],\n",
       "        [ 0.42943224,  0.10143775, -0.19244297, ...,  0.87659138,\n",
       "         -0.15979475,  0.74439585],\n",
       "        [ 0.42790341,  0.10110569, -0.21157819, ...,  0.87658101,\n",
       "         -0.17468312,  0.74519378]],\n",
       "\n",
       "       [[ 0.38855261,  0.17441459, -0.22123714, ...,  0.87199807,\n",
       "          0.09698085,  0.72984916],\n",
       "        [ 0.37399793,  0.17828743, -0.25825745, ...,  0.85410583,\n",
       "          0.21827859,  0.70169073],\n",
       "        [ 0.36673674,  0.17311949, -0.26217201, ...,  0.85722077,\n",
       "          0.06397771,  0.68321782],\n",
       "        ...,\n",
       "        [ 0.44784743,  0.09767452, -0.19296788, ...,  0.87692457,\n",
       "         -0.21121316,  0.7670278 ],\n",
       "        [ 0.45317891,  0.09736407, -0.19185957, ...,  0.87543255,\n",
       "         -0.13697369,  0.76021177],\n",
       "        [ 0.45885229,  0.09755029, -0.1449891 , ...,  0.875471  ,\n",
       "         -0.17986035,  0.7576201 ]],\n",
       "\n",
       "       [[ 0.38060215,  0.12118728, -0.34803388, ...,  0.87465268,\n",
       "         -0.18517356,  0.74403012],\n",
       "        [ 0.37014949,  0.12290786, -0.36867553, ...,  0.87451291,\n",
       "         -0.14505352,  0.73659492],\n",
       "        [ 0.37043723,  0.12141738, -0.35261196, ...,  0.87381482,\n",
       "         -0.10003949,  0.72956383],\n",
       "        ...,\n",
       "        [ 0.47088408,  0.09953246, -0.23520426, ...,  0.87722707,\n",
       "         -0.16220066,  0.74154997],\n",
       "        [ 0.47219941,  0.10036736, -0.23153716, ...,  0.876315  ,\n",
       "         -0.16953246,  0.74173707],\n",
       "        [ 0.47260204,  0.10194314, -0.25454128, ...,  0.87486804,\n",
       "         -0.16191772,  0.73782682]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f511257",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "y_categorical = to_categorical(y, num_classes=len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "965aeeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_categorical, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c89d0397",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87b81027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smayan/Desktop/AI-ML-DS/AI-and-ML-Course/.conda/lib/python3.11/site-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "I0000 00:00:1753696584.508591  375958 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9251 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# 1. TimeDistributed CNN block with BatchNorm\n",
    "model.add(TimeDistributed(Conv1D(64, kernel_size=3, padding='same', activation='relu'),\n",
    "                          input_shape=(sequence_length, X.shape[2], 1)))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "\n",
    "# 2. Second CNN block\n",
    "model.add(TimeDistributed(Conv1D(128, kernel_size=3, padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model.add(TimeDistributed(Dropout(0.3)))\n",
    "\n",
    "# 3. Flatten before LSTM\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "# 4. Bidirectional LSTM layers\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True, dropout=0.4, recurrent_dropout=0.2)))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=False, dropout=0.4, recurrent_dropout=0.2)))\n",
    "\n",
    "# 5. Fully Connected\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(actions), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32fd2cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = f'logs/cricket_model_{timestamp}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60e9eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcdffca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, update_freq='epoch'),\n",
    "    EarlyStopping(patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-7)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38526620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753696596.163880  376250 cuda_dnn.cc:529] Loaded cuDNN version 91100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 141ms/step - accuracy: 0.1315 - loss: 2.5680 - val_accuracy: 0.4180 - val_loss: 1.8194 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 120ms/step - accuracy: 0.3247 - loss: 1.8338 - val_accuracy: 0.5874 - val_loss: 1.1405 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 119ms/step - accuracy: 0.5081 - loss: 1.3227 - val_accuracy: 0.7678 - val_loss: 0.6720 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 117ms/step - accuracy: 0.6923 - loss: 0.9466 - val_accuracy: 0.8497 - val_loss: 0.4906 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 116ms/step - accuracy: 0.7428 - loss: 0.8074 - val_accuracy: 0.9071 - val_loss: 0.3059 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 122ms/step - accuracy: 0.8328 - loss: 0.5398 - val_accuracy: 0.9208 - val_loss: 0.2642 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 119ms/step - accuracy: 0.8602 - loss: 0.4677 - val_accuracy: 0.9044 - val_loss: 0.2889 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 121ms/step - accuracy: 0.8741 - loss: 0.3888 - val_accuracy: 0.9454 - val_loss: 0.1773 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 120ms/step - accuracy: 0.9022 - loss: 0.3147 - val_accuracy: 0.9372 - val_loss: 0.1971 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 121ms/step - accuracy: 0.9170 - loss: 0.2542 - val_accuracy: 0.9399 - val_loss: 0.2257 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 119ms/step - accuracy: 0.9243 - loss: 0.2699 - val_accuracy: 0.9344 - val_loss: 0.2036 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 119ms/step - accuracy: 0.9254 - loss: 0.2396 - val_accuracy: 0.9399 - val_loss: 0.1743 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 120ms/step - accuracy: 0.9408 - loss: 0.1991 - val_accuracy: 0.9536 - val_loss: 0.1465 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 121ms/step - accuracy: 0.9317 - loss: 0.1938 - val_accuracy: 0.9508 - val_loss: 0.1327 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 120ms/step - accuracy: 0.9436 - loss: 0.1814 - val_accuracy: 0.9536 - val_loss: 0.1224 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 120ms/step - accuracy: 0.9474 - loss: 0.1857 - val_accuracy: 0.9590 - val_loss: 0.1365 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 121ms/step - accuracy: 0.9619 - loss: 0.1191 - val_accuracy: 0.9727 - val_loss: 0.1205 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 120ms/step - accuracy: 0.9642 - loss: 0.1176 - val_accuracy: 0.9617 - val_loss: 0.1335 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 120ms/step - accuracy: 0.9649 - loss: 0.1365 - val_accuracy: 0.9727 - val_loss: 0.0970 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 121ms/step - accuracy: 0.9614 - loss: 0.1085 - val_accuracy: 0.9727 - val_loss: 0.0858 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 122ms/step - accuracy: 0.9667 - loss: 0.0964 - val_accuracy: 0.9699 - val_loss: 0.1178 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 120ms/step - accuracy: 0.9587 - loss: 0.1266 - val_accuracy: 0.9344 - val_loss: 0.1994 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 120ms/step - accuracy: 0.9712 - loss: 0.1003 - val_accuracy: 0.9617 - val_loss: 0.1615 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 121ms/step - accuracy: 0.9559 - loss: 0.1377 - val_accuracy: 0.9781 - val_loss: 0.0703 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 120ms/step - accuracy: 0.9613 - loss: 0.1079 - val_accuracy: 0.9617 - val_loss: 0.0999 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 119ms/step - accuracy: 0.9675 - loss: 0.0981 - val_accuracy: 0.9672 - val_loss: 0.1397 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 119ms/step - accuracy: 0.9526 - loss: 0.1427 - val_accuracy: 0.9617 - val_loss: 0.1474 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 116ms/step - accuracy: 0.9605 - loss: 0.1112 - val_accuracy: 0.9672 - val_loss: 0.1037 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 117ms/step - accuracy: 0.9518 - loss: 0.1694 - val_accuracy: 0.9727 - val_loss: 0.0660 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 115ms/step - accuracy: 0.9717 - loss: 0.0903 - val_accuracy: 0.9781 - val_loss: 0.0708 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 115ms/step - accuracy: 0.9765 - loss: 0.0720 - val_accuracy: 0.9699 - val_loss: 0.1164 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 116ms/step - accuracy: 0.9767 - loss: 0.0780 - val_accuracy: 0.9781 - val_loss: 0.0633 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 116ms/step - accuracy: 0.9837 - loss: 0.0544 - val_accuracy: 0.9836 - val_loss: 0.0491 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 117ms/step - accuracy: 0.9836 - loss: 0.0567 - val_accuracy: 0.9863 - val_loss: 0.0683 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 116ms/step - accuracy: 0.9840 - loss: 0.0417 - val_accuracy: 0.9699 - val_loss: 0.0761 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 115ms/step - accuracy: 0.9729 - loss: 0.1074 - val_accuracy: 0.9781 - val_loss: 0.0716 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 115ms/step - accuracy: 0.9846 - loss: 0.0466 - val_accuracy: 0.9754 - val_loss: 0.0773 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 116ms/step - accuracy: 0.9852 - loss: 0.0447 - val_accuracy: 0.9836 - val_loss: 0.0724 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 115ms/step - accuracy: 0.9946 - loss: 0.0188 - val_accuracy: 0.9809 - val_loss: 0.0651 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 116ms/step - accuracy: 0.9844 - loss: 0.0422 - val_accuracy: 0.9672 - val_loss: 0.1372 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 116ms/step - accuracy: 0.9876 - loss: 0.0530 - val_accuracy: 0.9863 - val_loss: 0.0563 - learning_rate: 0.0010\n",
      "Epoch 42/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 114ms/step - accuracy: 0.9825 - loss: 0.0587 - val_accuracy: 0.9836 - val_loss: 0.0700 - learning_rate: 0.0010\n",
      "Epoch 43/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 116ms/step - accuracy: 0.9838 - loss: 0.0453 - val_accuracy: 0.9645 - val_loss: 0.1481 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f14465f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9836\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc4cd51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('2ndbest_cricket_pose_mode_simple_even.h5')\n",
    "model.save('2ndbest_cricket_pose_model_simple_even.keras')\n",
    "np.save('cricket_label_map.npy', label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68b73e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "I0000 00:00:1753697679.468058  375958 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1753697679.511484  510271 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 570.172.08), renderer: NVIDIA GeForce RTX 4070 SUPER/PCIe/SSE2\n",
      "W0000 00:00:1753697679.542786  510248 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753697679.560045  510253 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753697679.561387  510270 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753697679.561648  510253 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753697679.562218  510260 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753697679.565491  510258 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753697679.570139  510254 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753697679.570906  510251 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Load trained model and label map\n",
    "model = tf.keras.models.load_model('cricket_pose_mode_simple_even.h5')\n",
    "label_map = np.load('cricket_label_map.npy', allow_pickle=True).item()\n",
    "actions = list(label_map.keys())\n",
    "\n",
    "# Variables for prediction\n",
    "sequence = []\n",
    "sequence_length = 30\n",
    "threshold = 0.9\n",
    "\n",
    "# Start webcam\n",
    "cap = cv.VideoCapture('/home/smayan/Desktop/Cricket Pose Estimation /Model Training/Test videos/test3.mp4')\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Resize for consistent input\n",
    "        # frame = cv.resize(frame, (640, 480))\n",
    "\n",
    "        # Detection\n",
    "        image = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = holistic.process(image)\n",
    "        image.flags.writeable = True\n",
    "        image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "\n",
    "        # Draw landmarks\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "            )\n",
    "\n",
    "        # Extract keypoints\n",
    "        if results.pose_landmarks:\n",
    "            keypoints = np.array([[res.x, res.y, res.z, res.visibility]\n",
    "                                  for res in results.pose_landmarks.landmark]).flatten()\n",
    "        else:\n",
    "            keypoints = np.zeros(33*4)\n",
    "\n",
    "        # Append to sequence\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-sequence_length:]\n",
    "\n",
    "        if len(sequence) == sequence_length:\n",
    "            input_seq = np.expand_dims(np.array(sequence), axis=0)\n",
    "            input_seq = input_seq.reshape(1, sequence_length, -1, 1)\n",
    "\n",
    "            # Predict\n",
    "            res = model.predict(input_seq, verbose=0)[0]\n",
    "            predicted_action = actions[np.argmax(res)]\n",
    "            confidence = np.max(res)\n",
    "\n",
    "            # Show prediction\n",
    "            if confidence > threshold:\n",
    "                cv.putText(image, f'{predicted_action}: {confidence:.2f}',\n",
    "                           (10, 50), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            # Show probabilities\n",
    "            for i, (action, prob) in enumerate(zip(actions, res)):\n",
    "                y_pos = 100 + i * 30\n",
    "                cv.rectangle(image, (10, y_pos), (int(prob * 300) + 10, y_pos + 25), (0, 255, 0), -1)\n",
    "                cv.putText(image, f'{action}: {prob:.2f}', (15, y_pos + 18),\n",
    "                           cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "        # Show output\n",
    "        cv.imshow('Cricket Pose Estimation', image)\n",
    "\n",
    "        # Quit\n",
    "        if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347c98bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f1a467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
