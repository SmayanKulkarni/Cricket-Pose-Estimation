{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ecc33d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten, TimeDistributed\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dac69f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "420e233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "689b1a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw pose connections for cricket analysis\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28b93b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    \"\"\"Extract pose keypoints - focusing on pose for cricket shots\"\"\"\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ed86942",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/smayan/Desktop/Cricket Pose Estimation /Data'  # Your cricket dataset path\n",
    "sequence_length = 30  # Frames per sequence\n",
    "min_sequences_per_class = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "047f12a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cricket_data():\n",
    "    \"\"\"Load cricket shot data with variable number of videos per class\"\"\"\n",
    "    # Get all cricket shot classes from folder names\n",
    "    actions = np.array(sorted([folder for folder in os.listdir(DATA_PATH) \n",
    "                              if os.path.isdir(os.path.join(DATA_PATH, folder))]))\n",
    "    print(f\"Detected cricket shots: {actions}\")\n",
    "    \n",
    "    # Count videos per class\n",
    "    for action in actions:\n",
    "        action_path = os.path.join(DATA_PATH, action)\n",
    "        video_files = [f for f in os.listdir(action_path) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "        print(f\"{action}: {len(video_files)} videos\")\n",
    "    \n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdf07f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def extract_sequences_from_videos(actions):\n",
    "    \"\"\"Extract sequences from videos with data augmentation for classes with fewer videos\"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    label_map = {label: num for num, label in enumerate(actions)}\n",
    "    \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        for action in actions:\n",
    "            action_path = os.path.join(DATA_PATH, action)\n",
    "            video_files = [f for f in os.listdir(action_path) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "            \n",
    "            action_sequences = []\n",
    "            \n",
    "            for video_file in tqdm(video_files, desc=f\"Processing {action}\"):\n",
    "                video_path = os.path.join(action_path, video_file)\n",
    "                cap = cv.VideoCapture(video_path)\n",
    "                \n",
    "                # Get total frames in video\n",
    "                total_frames = int(cap.get(cv.CAP_PROP_FRAME_COUNT))\n",
    "                \n",
    "                # Extract multiple sequences from each video using sliding window\n",
    "                stride = max(1, sequence_length // 4)  # Overlapping sequences\n",
    "                \n",
    "                for start_frame in tqdm(range(0, total_frames - sequence_length + 1, stride), \n",
    "                        desc=f\"Frames in {video_file}\", leave=False):\n",
    "                    cap.set(cv.CAP_PROP_POS_FRAMES, start_frame)\n",
    "                    sequence = []\n",
    "                    \n",
    "                    for frame_idx in range(sequence_length):\n",
    "                        ret, frame = cap.read()\n",
    "                        if not ret:\n",
    "                            break\n",
    "                        \n",
    "                        # Resize frame for consistency\n",
    "                        frame = cv.resize(frame, (640, 480))\n",
    "                        \n",
    "                        _, results = mediapipe_detection(frame, holistic)\n",
    "                        keypoints = extract_keypoints(results)\n",
    "                        sequence.append(keypoints)\n",
    "                    \n",
    "                    if len(sequence) == sequence_length:\n",
    "                        action_sequences.append(sequence)\n",
    "                \n",
    "                cap.release()\n",
    "            \n",
    "            # Data augmentation for classes with fewer sequences\n",
    "            while len(action_sequences) < min_sequences_per_class:\n",
    "                # Add augmented versions (you can implement more sophisticated augmentation)\n",
    "                if action_sequences:\n",
    "                    # Simple augmentation: add noise\n",
    "                    original_seq = np.array(action_sequences[len(action_sequences) % len(action_sequences)])\n",
    "                    noise = np.random.normal(0, 0.01, original_seq.shape)\n",
    "                    augmented_seq = original_seq + noise\n",
    "                    action_sequences.append(augmented_seq.tolist())\n",
    "            \n",
    "            # Add sequences and labels\n",
    "            for seq in action_sequences:\n",
    "                sequences.append(seq)\n",
    "                labels.append(label_map[action])\n",
    "            \n",
    "            print(f\"Generated {len(action_sequences)} sequences for {action}\")\n",
    "    \n",
    "    return np.array(sequences), np.array(labels), label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac4ce228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hybrid_cnn_lstm_model(input_shape, num_classes):\n",
    "    \"\"\"Create a Hybrid CNN-LSTM model for cricket pose estimation\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # CNN layers for spatial feature extraction\n",
    "    model.add(TimeDistributed(Conv1D(64, kernel_size=3, activation='relu'), \n",
    "                             input_shape=input_shape))\n",
    "    model.add(TimeDistributed(Conv1D(64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv1D(128, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Conv1D(128, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    # Flatten the CNN output for LSTM\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    \n",
    "    # LSTM layers for temporal sequence modeling\n",
    "    model.add(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))\n",
    "    model.add(LSTM(64, return_sequences=False, dropout=0.3, recurrent_dropout=0.3))\n",
    "    \n",
    "    # Dense layers for classification\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "754c20dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cricket_model():\n",
    "    # Load data\n",
    "    actions = load_cricket_data()\n",
    "    print(\"Extracting sequences from videos...\")\n",
    "    X, y, label_map = extract_sequences_from_videos(actions)\n",
    "    \n",
    "    print(f\"Dataset shape: {X.shape}\")\n",
    "    print(f\"Labels shape: {y.shape}\")\n",
    "    \n",
    "    # Reshape X for CNN input (samples, timesteps, features, 1)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "    \n",
    "    # Convert labels to categorical\n",
    "    y_categorical = to_categorical(y, num_classes=len(actions))\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_categorical, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape}\")\n",
    "    print(f\"Test set: {X_test.shape}\")\n",
    "    \n",
    "    # Handle class imbalance\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced', \n",
    "        classes=np.unique(y), \n",
    "        y=y\n",
    "    )\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    # Create model\n",
    "    model = create_hybrid_cnn_lstm_model(\n",
    "        input_shape=(sequence_length, X.shape[2], 1), \n",
    "        num_classes=len(actions)\n",
    "    )\n",
    "# Create timestamp for TensorBoard logs\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = f'logs/cricket_model_{timestamp}'\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "            TensorBoard(\n",
    "                log_dir=log_dir,\n",
    "                histogram_freq=1,\n",
    "                write_graph=True,\n",
    "                update_freq='epoch'\n",
    "            ),\n",
    "            EarlyStopping(patience=15, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-7)\n",
    "        ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=16,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    model.save('cricket_pose_model.h5')\n",
    "    model.save('cricket_pose_model.keras')\n",
    "    \n",
    "    # Save label mapping\n",
    "    np.save('cricket_label_map.npy', label_map)\n",
    "    \n",
    "    return model, history, label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f7ee5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_time_cricket_prediction():\n",
    "    \"\"\"Real-time cricket shot prediction\"\"\"\n",
    "    model = tf.keras.models.load_model('cricket_pose_model.h5')\n",
    "    label_map = np.load('cricket_label_map.npy', allow_pickle=True).item()\n",
    "    actions = list(label_map.keys())\n",
    "    \n",
    "    # Prediction variables\n",
    "    sequence = []\n",
    "    predictions = []\n",
    "    threshold = 0.7\n",
    "    \n",
    "    cap = cv.VideoCapture(0)  # Use webcam\n",
    "    \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Make detections\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            draw_styled_landmarks(image, results)\n",
    "            \n",
    "            # Extract keypoints\n",
    "            keypoints = extract_keypoints(results)\n",
    "            sequence.append(keypoints)\n",
    "            sequence = sequence[-sequence_length:]\n",
    "            \n",
    "            if len(sequence) == sequence_length:\n",
    "                # Reshape for model input\n",
    "                input_seq = np.expand_dims(np.array(sequence), axis=0)\n",
    "                input_seq = input_seq.reshape(1, sequence_length, -1, 1)\n",
    "                \n",
    "                # Make prediction\n",
    "                res = model.predict(input_seq, verbose=0)[0]\n",
    "                predicted_action = actions[np.argmax(res)]\n",
    "                confidence = np.max(res)\n",
    "                \n",
    "                # Display prediction\n",
    "                if confidence > threshold:\n",
    "                    cv.putText(image, f'{predicted_action}: {confidence:.2f}', \n",
    "                              (10, 50), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                \n",
    "                # Visualization of probabilities\n",
    "                for i, (action, prob) in enumerate(zip(actions, res)):\n",
    "                    y_pos = 100 + i * 30\n",
    "                    cv.rectangle(image, (10, y_pos), (int(prob * 300) + 10, y_pos + 25), \n",
    "                               (0, 255, 0), -1)\n",
    "                    cv.putText(image, f'{action}: {prob:.2f}', (15, y_pos + 18), \n",
    "                              cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "            \n",
    "            cv.imshow('Cricket Pose Estimation', image)\n",
    "            \n",
    "            if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "    \n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "833b1e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753628289.843167   15138 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1753628289.902044   17549 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 570.172.08), renderer: NVIDIA GeForce RTX 4070 SUPER/PCIe/SSE2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected cricket shots: ['Backfoot punch' 'Cover drive' 'Cut Shot' 'FBD' 'Flick'\n",
      " 'Front Food defence' 'On Drive' 'Pull Shot' 'Reverse Sweep'\n",
      " 'Straight Drive' 'Sweep' 'Uppercut' 'loft']\n",
      "Backfoot punch: 19 videos\n",
      "Cover drive: 29 videos\n",
      "Cut Shot: 43 videos\n",
      "FBD: 15 videos\n",
      "Flick: 22 videos\n",
      "Front Food defence: 32 videos\n",
      "On Drive: 40 videos\n",
      "Pull Shot: 40 videos\n",
      "Reverse Sweep: 30 videos\n",
      "Straight Drive: 25 videos\n",
      "Sweep: 27 videos\n",
      "Uppercut: 29 videos\n",
      "loft: 31 videos\n",
      "Extracting sequences from videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Backfoot punch:   0%|          | 0/19 [00:00<?, ?it/s]W0000 00:00:1753628289.934759   17522 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753628289.951133   17534 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753628289.952187   17528 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753628289.952442   17531 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753628289.952510   17521 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753628289.958027   17527 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753628289.962560   17543 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753628289.967258   17526 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "Processing Backfoot punch: 100%|██████████| 19/19 [01:16<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 112 sequences for Backfoot punch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Cover drive: 100%|██████████| 29/29 [01:50<00:00,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 152 sequences for Cover drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Cut Shot: 100%|██████████| 43/43 [02:19<00:00,  3.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 193 sequences for Cut Shot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing FBD: 100%|██████████| 15/15 [01:06<00:00,  4.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 91 sequences for FBD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Flick: 100%|██████████| 22/22 [01:21<00:00,  3.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 112 sequences for Flick\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Front Food defence: 100%|██████████| 32/32 [02:17<00:00,  4.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 186 sequences for Front Food defence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing On Drive: 100%|██████████| 40/40 [01:52<00:00,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 155 sequences for On Drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Pull Shot: 100%|██████████| 40/40 [02:31<00:00,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 212 sequences for Pull Shot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Reverse Sweep: 100%|██████████| 30/30 [02:26<00:00,  4.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 199 sequences for Reverse Sweep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Straight Drive: 100%|██████████| 25/25 [02:18<00:00,  5.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 191 sequences for Straight Drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sweep: 100%|██████████| 27/27 [01:56<00:00,  4.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 159 sequences for Sweep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Uppercut: 100%|██████████| 29/29 [01:45<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 146 sequences for Uppercut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing loft: 100%|██████████| 31/31 [02:12<00:00,  4.27s/it]\n",
      "/home/smayan/Desktop/AI-ML-DS/AI-and-ML-Course/.conda/lib/python3.11/site-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "I0000 00:00:1753629805.539658   15138 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9350 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 184 sequences for loft\n",
      "Dataset shape: (2092, 30, 132)\n",
      "Labels shape: (2092,)\n",
      "Training set: (1673, 30, 132, 1)\n",
      "Test set: (419, 30, 132, 1)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'datetime' has no attribute 'now'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     model, history, label_map \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_cricket_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 38\u001b[0m, in \u001b[0;36mtrain_cricket_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     model \u001b[38;5;241m=\u001b[39m create_hybrid_cnn_lstm_model(\n\u001b[1;32m     34\u001b[0m         input_shape\u001b[38;5;241m=\u001b[39m(sequence_length, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m1\u001b[39m), \n\u001b[1;32m     35\u001b[0m         num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(actions)\n\u001b[1;32m     36\u001b[0m     )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Create timestamp for TensorBoard logs\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     timestamp \u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnow\u001b[49m()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m     log_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogs/cricket_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Compile model\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'datetime' has no attribute 'now'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Train the model\n",
    "    model, history, label_map = train_cricket_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72ee1827",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mX\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754fabf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
