{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef7384f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-27 20:16:03.701215: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-27 20:16:03.708034: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753627563.716105    9550 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753627563.718472    9550 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753627563.724704    9550 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753627563.724713    9550 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753627563.724714    9550 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753627563.724715    9550 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-27 20:16:03.726952: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected cricket shots: ['Backfoot punch' 'Cover drive' 'Cut Shot' 'FBD' 'Flick'\n",
      " 'Front Food defence' 'On Drive' 'Pull Shot' 'Reverse Sweep'\n",
      " 'Straight Drive' 'Sweep' 'Uppercut' 'loft']\n",
      "Backfoot punch: 19 videos\n",
      "Cover drive: 29 videos\n",
      "Cut Shot: 43 videos\n",
      "FBD: 15 videos\n",
      "Flick: 22 videos\n",
      "Front Food defence: 32 videos\n",
      "On Drive: 40 videos\n",
      "Pull Shot: 40 videos\n",
      "Reverse Sweep: 30 videos\n",
      "Straight Drive: 25 videos\n",
      "Sweep: 27 videos\n",
      "Uppercut: 29 videos\n",
      "loft: 31 videos\n",
      "Extracting sequences from videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1753627564.899793    9550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1753627564.957533   12295 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 570.172.08), renderer: NVIDIA GeForce RTX 4070 SUPER/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1753627564.991091   12274 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753627565.020352   12294 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753627565.021698   12271 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753627565.022071   12290 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753627565.022075   12282 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753627565.026178   12285 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753627565.032881   12289 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753627565.035322   12288 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753627565.038974   12274 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 292\u001b[0m\n\u001b[1;32m    288\u001b[0m     cv\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     model, history, label_map \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_cricket_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;66;03m# Optionally run real-time prediction\u001b[39;00m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;66;03m# real_time_cricket_prediction()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 159\u001b[0m, in \u001b[0;36mtrain_cricket_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m actions \u001b[38;5;241m=\u001b[39m load_cricket_data()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting sequences from videos...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 159\u001b[0m X, y, label_map \u001b[38;5;241m=\u001b[39m \u001b[43mextract_sequences_from_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabels shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 94\u001b[0m, in \u001b[0;36mextract_sequences_from_videos\u001b[0;34m(actions)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Resize frame for consistency\u001b[39;00m\n\u001b[1;32m     92\u001b[0m frame \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mresize(frame, (\u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m480\u001b[39m))\n\u001b[0;32m---> 94\u001b[0m _, results \u001b[38;5;241m=\u001b[39m \u001b[43mmediapipe_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mholistic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m extract_keypoints(results)\n\u001b[1;32m     96\u001b[0m sequence\u001b[38;5;241m.\u001b[39mappend(keypoints)\n",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m, in \u001b[0;36mmediapipe_detection\u001b[0;34m(image, model)\u001b[0m\n\u001b[1;32m     20\u001b[0m image \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mcvtColor(image, cv\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     21\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     24\u001b[0m image \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mcvtColor(image, cv\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR)\n",
      "File \u001b[0;32m~/Desktop/AI-ML-DS/AI-and-ML-Course/.conda/lib/python3.11/site-packages/mediapipe/python/solutions/holistic.py:160\u001b[0m, in \u001b[0;36mHolistic.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[1;32m    137\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks, left and right hand landmarks, and face landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AI-ML-DS/AI-and-ML-Course/.conda/lib/python3.11/site-packages/mediapipe/python/solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[1;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten, TimeDistributed\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "\n",
    "# MediaPipe setup\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw pose connections for cricket analysis\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    \"\"\"Extract pose keypoints - focusing on pose for cricket shots\"\"\"\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    return pose\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = '/home/smayan/Desktop/Cricket Pose Estimation /Data'  # Your cricket dataset path\n",
    "sequence_length = 30  # Frames per sequence\n",
    "min_sequences_per_class = 10  # Minimum sequences to generate per class\n",
    "\n",
    "def load_cricket_data():\n",
    "    \"\"\"Load cricket shot data with variable number of videos per class\"\"\"\n",
    "    # Get all cricket shot classes from folder names\n",
    "    actions = np.array(sorted([folder for folder in os.listdir(DATA_PATH) \n",
    "                              if os.path.isdir(os.path.join(DATA_PATH, folder))]))\n",
    "    print(f\"Detected cricket shots: {actions}\")\n",
    "    \n",
    "    # Count videos per class\n",
    "    for action in actions:\n",
    "        action_path = os.path.join(DATA_PATH, action)\n",
    "        video_files = [f for f in os.listdir(action_path) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "        print(f\"{action}: {len(video_files)} videos\")\n",
    "    \n",
    "    return actions\n",
    "\n",
    "def extract_sequences_from_videos(actions):\n",
    "    \"\"\"Extract sequences from videos with data augmentation for classes with fewer videos\"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    label_map = {label: num for num, label in enumerate(actions)}\n",
    "    \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        for action in actions:\n",
    "            action_path = os.path.join(DATA_PATH, action)\n",
    "            video_files = [f for f in os.listdir(action_path) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "            \n",
    "            action_sequences = []\n",
    "            \n",
    "            for video_file in video_files:\n",
    "                video_path = os.path.join(action_path, video_file)\n",
    "                cap = cv.VideoCapture(video_path)\n",
    "                \n",
    "                # Get total frames in video\n",
    "                total_frames = int(cap.get(cv.CAP_PROP_FRAME_COUNT))\n",
    "                \n",
    "                # Extract multiple sequences from each video using sliding window\n",
    "                stride = max(1, sequence_length // 4)  # Overlapping sequences\n",
    "                \n",
    "                for start_frame in range(0, total_frames - sequence_length + 1, stride):\n",
    "                    cap.set(cv.CAP_PROP_POS_FRAMES, start_frame)\n",
    "                    sequence = []\n",
    "                    \n",
    "                    for frame_idx in range(sequence_length):\n",
    "                        ret, frame = cap.read()\n",
    "                        if not ret:\n",
    "                            break\n",
    "                        \n",
    "                        # Resize frame for consistency\n",
    "                        frame = cv.resize(frame, (640, 480))\n",
    "                        \n",
    "                        _, results = mediapipe_detection(frame, holistic)\n",
    "                        keypoints = extract_keypoints(results)\n",
    "                        sequence.append(keypoints)\n",
    "                    \n",
    "                    if len(sequence) == sequence_length:\n",
    "                        action_sequences.append(sequence)\n",
    "                \n",
    "                cap.release()\n",
    "            \n",
    "            # Data augmentation for classes with fewer sequences\n",
    "            while len(action_sequences) < min_sequences_per_class:\n",
    "                # Add augmented versions (you can implement more sophisticated augmentation)\n",
    "                if action_sequences:\n",
    "                    # Simple augmentation: add noise\n",
    "                    original_seq = np.array(action_sequences[len(action_sequences) % len(action_sequences)])\n",
    "                    noise = np.random.normal(0, 0.01, original_seq.shape)\n",
    "                    augmented_seq = original_seq + noise\n",
    "                    action_sequences.append(augmented_seq.tolist())\n",
    "            \n",
    "            # Add sequences and labels\n",
    "            for seq in action_sequences:\n",
    "                sequences.append(seq)\n",
    "                labels.append(label_map[action])\n",
    "            \n",
    "            print(f\"Generated {len(action_sequences)} sequences for {action}\")\n",
    "    \n",
    "    return np.array(sequences), np.array(labels), label_map\n",
    "\n",
    "def create_hybrid_cnn_lstm_model(input_shape, num_classes):\n",
    "    \"\"\"Create a Hybrid CNN-LSTM model for cricket pose estimation\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # CNN layers for spatial feature extraction\n",
    "    model.add(TimeDistributed(Conv1D(64, kernel_size=3, activation='relu'), \n",
    "                             input_shape=input_shape))\n",
    "    model.add(TimeDistributed(Conv1D(64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv1D(128, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Conv1D(128, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    # Flatten the CNN output for LSTM\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    \n",
    "    # LSTM layers for temporal sequence modeling\n",
    "    model.add(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))\n",
    "    model.add(LSTM(64, return_sequences=False, dropout=0.3, recurrent_dropout=0.3))\n",
    "    \n",
    "    # Dense layers for classification\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Main training pipeline\n",
    "def train_cricket_model():\n",
    "    # Load data\n",
    "    actions = load_cricket_data()\n",
    "    print(\"Extracting sequences from videos...\")\n",
    "    X, y, label_map = extract_sequences_from_videos(actions)\n",
    "    \n",
    "    print(f\"Dataset shape: {X.shape}\")\n",
    "    print(f\"Labels shape: {y.shape}\")\n",
    "    \n",
    "    # Reshape X for CNN input (samples, timesteps, features, 1)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "    \n",
    "    # Convert labels to categorical\n",
    "    y_categorical = to_categorical(y, num_classes=len(actions))\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_categorical, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape}\")\n",
    "    print(f\"Test set: {X_test.shape}\")\n",
    "    \n",
    "    # Handle class imbalance\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced', \n",
    "        classes=np.unique(y), \n",
    "        y=y\n",
    "    )\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    # Create model\n",
    "    model = create_hybrid_cnn_lstm_model(\n",
    "        input_shape=(sequence_length, X.shape[2], 1), \n",
    "        num_classes=len(actions)\n",
    "    )\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        TensorBoard(log_dir='logs/cricket_model'),\n",
    "        EarlyStopping(patience=15, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-7)\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=10,\n",
    "        batch_size=16,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    model.save('cricket_pose_model.h5')\n",
    "    model.save('cricket_pose_model.keras')\n",
    "    \n",
    "    # Save label mapping\n",
    "    np.save('cricket_label_map.npy', label_map)\n",
    "    \n",
    "    return model, history, label_map\n",
    "\n",
    "# Real-time prediction function\n",
    "def real_time_cricket_prediction():\n",
    "    \"\"\"Real-time cricket shot prediction\"\"\"\n",
    "    model = tf.keras.models.load_model('cricket_pose_model.h5')\n",
    "    label_map = np.load('cricket_label_map.npy', allow_pickle=True).item()\n",
    "    actions = list(label_map.keys())\n",
    "    \n",
    "    # Prediction variables\n",
    "    sequence = []\n",
    "    predictions = []\n",
    "    threshold = 0.7\n",
    "    \n",
    "    cap = cv.VideoCapture(0)  # Use webcam\n",
    "    \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Make detections\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            draw_styled_landmarks(image, results)\n",
    "            \n",
    "            # Extract keypoints\n",
    "            keypoints = extract_keypoints(results)\n",
    "            sequence.append(keypoints)\n",
    "            sequence = sequence[-sequence_length:]\n",
    "            \n",
    "            if len(sequence) == sequence_length:\n",
    "                # Reshape for model input\n",
    "                input_seq = np.expand_dims(np.array(sequence), axis=0)\n",
    "                input_seq = input_seq.reshape(1, sequence_length, -1, 1)\n",
    "                \n",
    "                # Make prediction\n",
    "                res = model.predict(input_seq, verbose=0)[0]\n",
    "                predicted_action = actions[np.argmax(res)]\n",
    "                confidence = np.max(res)\n",
    "                \n",
    "                # Display prediction\n",
    "                if confidence > threshold:\n",
    "                    cv.putText(image, f'{predicted_action}: {confidence:.2f}', \n",
    "                              (10, 50), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                \n",
    "                # Visualization of probabilities\n",
    "                for i, (action, prob) in enumerate(zip(actions, res)):\n",
    "                    y_pos = 100 + i * 30\n",
    "                    cv.rectangle(image, (10, y_pos), (int(prob * 300) + 10, y_pos + 25), \n",
    "                               (0, 255, 0), -1)\n",
    "                    cv.putText(image, f'{action}: {prob:.2f}', (15, y_pos + 18), \n",
    "                              cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "            \n",
    "            cv.imshow('Cricket Pose Estimation', image)\n",
    "            \n",
    "            if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "    \n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Train the model\n",
    "    model, history, label_map = train_cricket_model()\n",
    "    \n",
    "    # Optionally run real-time prediction\n",
    "    # real_time_cricket_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb532c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
