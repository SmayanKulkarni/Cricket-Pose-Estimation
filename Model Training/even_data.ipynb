{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05a62929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 15:55:21.126416: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-28 15:55:21.133319: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753698321.141208   15755 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753698321.143596   15755 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753698321.149788   15755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753698321.149796   15755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753698321.149797   15755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753698321.149798   15755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-28 15:55:21.151845: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94ae33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b6952f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/smayan/Desktop/Cricket Pose Estimation /Data'\n",
    "sequence_length = 30\n",
    "min_sequences_per_class = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6830cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected cricket shots: ['Backfoot punch' 'Cover drive' 'Cut Shot' 'FBD' 'Flick'\n",
      " 'Front Food defence' 'On Drive' 'Pull Shot' 'Reverse Sweep' 'Stance'\n",
      " 'Straight Drive' 'Sweep' 'Uppercut' 'loft']\n",
      "Backfoot punch: 19 videos\n",
      "Cover drive: 25 videos\n",
      "Cut Shot: 25 videos\n",
      "FBD: 15 videos\n",
      "Flick: 22 videos\n",
      "Front Food defence: 25 videos\n",
      "On Drive: 25 videos\n",
      "Pull Shot: 25 videos\n",
      "Reverse Sweep: 25 videos\n",
      "Stance: 25 videos\n",
      "Straight Drive: 25 videos\n",
      "Sweep: 25 videos\n",
      "Uppercut: 25 videos\n",
      "loft: 25 videos\n"
     ]
    }
   ],
   "source": [
    "actions = np.array(sorted([folder for folder in os.listdir(DATA_PATH) \n",
    "                          if os.path.isdir(os.path.join(DATA_PATH, folder))]))\n",
    "print(f\"Detected cricket shots: {actions}\")\n",
    "for action in actions:\n",
    "    video_files = [f for f in os.listdir(os.path.join(DATA_PATH, action)) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "    video_files = video_files[:25]\n",
    "    print(f\"{action}: {len(video_files)} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f7ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "labels = []\n",
    "label_map = {label: num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80f81cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753698498.130242   15755 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1753698498.187942   17836 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 570.172.08), renderer: NVIDIA GeForce RTX 4070 SUPER/PCIe/SSE2\n",
      "Processing Backfoot punch:   0%|          | 0/19 [00:00<?, ?it/s]W0000 00:00:1753698498.221536   17812 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753698498.242115   17823 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753698498.243460   17809 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753698498.243569   17813 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753698498.243724   17816 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753698498.247817   17832 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753698498.248289   17819 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753698498.252750   17814 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "Processing Backfoot punch: 100%|██████████| 19/19 [01:15<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 112 sequences for Backfoot punch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Cover drive: 100%|██████████| 25/25 [01:34<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 132 sequences for Cover drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Cut Shot: 100%|██████████| 25/25 [01:20<00:00,  3.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 113 sequences for Cut Shot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing FBD: 100%|██████████| 15/15 [01:06<00:00,  4.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 91 sequences for FBD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Flick: 100%|██████████| 22/22 [01:21<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 112 sequences for Flick\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Front Food defence: 100%|██████████| 25/25 [01:48<00:00,  4.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 150 sequences for Front Food defence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing On Drive: 100%|██████████| 25/25 [01:07<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 94 sequences for On Drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Pull Shot: 100%|██████████| 25/25 [01:33<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 131 sequences for Pull Shot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Reverse Sweep: 100%|██████████| 25/25 [02:00<00:00,  4.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 165 sequences for Reverse Sweep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Stance:  12%|█▏        | 3/25 [00:10<01:22,  3.73s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x5b8864bcdd80] moov atom not found\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5b88618ca880] moov atom not found\n",
      "Processing Stance:  32%|███▏      | 8/25 [00:21<00:45,  2.66s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x5b885f413800] moov atom not found\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5b8862384ec0] moov atom not found\n",
      "Processing Stance:  64%|██████▍   | 16/25 [00:50<00:32,  3.64s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x5b8866a07580] moov atom not found\n",
      "Processing Stance:  80%|████████  | 20/25 [01:08<00:23,  4.65s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x5b88624f4e00] moov atom not found\n",
      "Processing Stance:  96%|█████████▌| 24/25 [01:20<00:03,  3.83s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x5b886278ac80] moov atom not found\n",
      "Processing Stance: 100%|██████████| 25/25 [01:20<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 112 sequences for Stance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Straight Drive: 100%|██████████| 25/25 [02:18<00:00,  5.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 191 sequences for Straight Drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sweep: 100%|██████████| 25/25 [01:49<00:00,  4.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 149 sequences for Sweep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Uppercut: 100%|██████████| 25/25 [01:31<00:00,  3.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 126 sequences for Uppercut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing loft: 100%|██████████| 25/25 [01:46<00:00,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 149 sequences for loft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    for action in actions:\n",
    "        action_path = os.path.join(DATA_PATH, action)\n",
    "        video_files = [f for f in os.listdir(action_path) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "        video_files = video_files[:25]\n",
    "        action_sequences = []\n",
    "\n",
    "        for video_file in tqdm(video_files, desc=f\"Processing {action}\"):\n",
    "            video_path = os.path.join(action_path, video_file)\n",
    "            cap = cv.VideoCapture(video_path)\n",
    "            total_frames = int(cap.get(cv.CAP_PROP_FRAME_COUNT))\n",
    "            stride = max(1, sequence_length // 4)\n",
    "\n",
    "            for start_frame in tqdm(range(0, total_frames - sequence_length + 1, stride), \n",
    "                                    desc=f\"Frames in {video_file}\", leave=False):\n",
    "                cap.set(cv.CAP_PROP_POS_FRAMES, start_frame)\n",
    "                sequence = []\n",
    "\n",
    "                for _ in range(sequence_length):\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                    frame = cv.resize(frame, (640, 480))\n",
    "                    image = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "                    image.flags.writeable = False\n",
    "                    results = holistic.process(image)\n",
    "                    image.flags.writeable = True\n",
    "                    image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "                    if results.pose_landmarks:\n",
    "                        pose = np.array([[res.x, res.y, res.z, res.visibility] \n",
    "                                         for res in results.pose_landmarks.landmark]).flatten()\n",
    "                    else:\n",
    "                        pose = np.zeros(33*4)\n",
    "                    sequence.append(pose)\n",
    "\n",
    "                if len(sequence) == sequence_length:\n",
    "                    action_sequences.append(sequence)\n",
    "\n",
    "            cap.release()\n",
    "\n",
    "        while len(action_sequences) < min_sequences_per_class:\n",
    "            if action_sequences:\n",
    "                original_seq = np.array(action_sequences[len(action_sequences) % len(action_sequences)])\n",
    "                noise = np.random.normal(0, 0.01, original_seq.shape)\n",
    "                augmented_seq = original_seq + noise\n",
    "                action_sequences.append(augmented_seq.tolist())\n",
    "\n",
    "        for seq in action_sequences:\n",
    "            sequences.append(seq)\n",
    "            labels.append(label_map[action])\n",
    "        print(f\"Generated {len(action_sequences)} sequences for {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "191ce2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1939, 30, 132)\n",
      "Labels shape: (1939,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(sequences)\n",
    "y = np.array(labels)\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "\n",
    "\n",
    "np.save('training_data.npy', X)\n",
    "np.save('labels.npy', y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9208210b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 5.64968348e-01,  2.49950171e-01,  3.38950045e-02, ...,\n",
       "          7.92152047e-01,  1.12857200e-01,  1.97137430e-01],\n",
       "        [ 5.64456105e-01,  2.49205589e-01,  1.35365976e-02, ...,\n",
       "          7.91108251e-01,  7.04743341e-02,  2.10779011e-01],\n",
       "        [ 5.63663423e-01,  2.49177113e-01,  1.81654505e-02, ...,\n",
       "          7.89731920e-01,  7.93914273e-02,  2.19110340e-01],\n",
       "        ...,\n",
       "        [ 3.96243960e-01,  3.10435861e-01,  1.03527762e-01, ...,\n",
       "          8.21304440e-01, -1.22316718e-01,  6.95255220e-01],\n",
       "        [ 4.02454674e-01,  3.31085622e-01,  3.33393335e-01, ...,\n",
       "          7.87335753e-01, -4.52745110e-01,  7.09456921e-01],\n",
       "        [ 3.95582497e-01,  3.26448768e-01,  2.72976100e-01, ...,\n",
       "          7.80987144e-01, -2.21305177e-01,  7.36743331e-01]],\n",
       "\n",
       "       [[ 5.35325587e-01,  2.80128688e-01,  9.64741707e-02, ...,\n",
       "          8.29995155e-01, -7.46523682e-03,  6.73663378e-01],\n",
       "        [ 5.31645834e-01,  2.69057244e-01,  3.76071148e-02, ...,\n",
       "          8.31327558e-01,  1.41000867e-01,  6.22527421e-01],\n",
       "        [ 5.23291171e-01,  2.69600093e-01, -3.12610194e-02, ...,\n",
       "          8.19697440e-01,  1.68077692e-01,  5.94806731e-01],\n",
       "        ...,\n",
       "        [ 3.56972665e-01,  2.96188474e-01,  1.75448447e-01, ...,\n",
       "          7.55894244e-01, -4.85318840e-01,  7.72768497e-01],\n",
       "        [ 3.60590488e-01,  2.90922046e-01,  2.59100050e-01, ...,\n",
       "          7.56472766e-01, -3.89072776e-01,  7.56494522e-01],\n",
       "        [ 3.63950938e-01,  2.66659081e-01,  2.81923771e-01, ...,\n",
       "          7.48510420e-01, -5.37395120e-01,  7.24182665e-01]],\n",
       "\n",
       "       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 3.31266522e-01,  2.70651668e-01,  2.84001499e-01, ...,\n",
       "          7.14538932e-01, -2.14519665e-01,  6.73392534e-01],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 4.64689076e-01,  7.00437009e-01,  2.99138248e-01, ...,\n",
       "          1.84062555e-01, -2.03751057e-01,  7.94228613e-01],\n",
       "        [ 4.95024055e-01,  6.26546919e-01,  4.25878674e-01, ...,\n",
       "          2.01476291e-01, -3.18216234e-01,  8.02779317e-01],\n",
       "        [ 4.54203784e-01,  6.70656383e-01,  3.32212120e-01, ...,\n",
       "          2.07192183e-01, -2.76573092e-01,  8.15452337e-01],\n",
       "        ...,\n",
       "        [ 3.24972451e-01,  7.41988838e-01,  9.63644832e-02, ...,\n",
       "          6.33896515e-02,  2.45329589e-01,  8.85374188e-01],\n",
       "        [ 3.22339982e-01,  7.35248566e-01,  4.51443717e-02, ...,\n",
       "          5.27503230e-02,  1.65588185e-01,  8.81571233e-01],\n",
       "        [ 3.21487129e-01,  7.50124395e-01,  7.65237361e-02, ...,\n",
       "          4.57019769e-02,  1.02712713e-01,  8.74008238e-01]],\n",
       "\n",
       "       [[ 3.63313377e-01,  7.51267672e-01,  7.28079826e-02, ...,\n",
       "          2.18758553e-01,  1.42728522e-01,  8.58004630e-01],\n",
       "        [ 3.79456699e-01,  6.66452348e-01,  1.64447010e-01, ...,\n",
       "          2.33848259e-01,  9.99896377e-02,  8.34315717e-01],\n",
       "        [ 3.73704970e-01,  6.69690907e-01,  1.51720122e-01, ...,\n",
       "          1.18243285e-01, -9.34109017e-02,  8.37987661e-01],\n",
       "        ...,\n",
       "        [ 3.25630248e-01,  7.78468847e-01,  6.17560335e-02, ...,\n",
       "          2.69949399e-02,  5.76169677e-02,  8.65180254e-01],\n",
       "        [ 3.25910151e-01,  7.74781525e-01,  6.59052432e-02, ...,\n",
       "          2.99398191e-02, -4.78988774e-02,  8.58680010e-01],\n",
       "        [ 3.33946019e-01,  7.79987276e-01,  7.26453215e-02, ...,\n",
       "          2.73947064e-02, -1.22659087e-01,  8.53569865e-01]],\n",
       "\n",
       "       [[ 3.45960468e-01,  7.58327603e-01,  7.04807192e-02, ...,\n",
       "          6.18009530e-02, -5.15696127e-04,  8.61758828e-01],\n",
       "        [ 3.43944609e-01,  7.51770854e-01,  8.54084641e-02, ...,\n",
       "          6.07101470e-02,  6.23035319e-02,  8.63520324e-01],\n",
       "        [ 3.46508175e-01,  7.34499633e-01,  1.15891099e-01, ...,\n",
       "          5.99995703e-02,  1.25051159e-02,  8.65440965e-01],\n",
       "        ...,\n",
       "        [ 3.46694052e-01,  7.34750688e-01,  4.53531832e-01, ...,\n",
       "          3.18868570e-02, -3.39544624e-01,  8.24074566e-01],\n",
       "        [ 3.45697433e-01,  7.38551795e-01,  3.42815250e-01, ...,\n",
       "          3.13881338e-02, -4.02027108e-02,  8.03316534e-01],\n",
       "        [ 3.44499409e-01,  7.36153066e-01,  3.18772554e-01, ...,\n",
       "          3.72293591e-02, -9.02492777e-02,  7.62145102e-01]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d591a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "y_categorical = to_categorical(y, num_classes=len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82bb75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "959e84bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_categorical, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb64909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8a4b82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smayan/Desktop/AI-ML-DS/AI-and-ML-Course/.conda/lib/python3.11/site-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "I0000 00:00:1753699824.070778   15755 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9077 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# TimeDistributed CNN - 1 block only\n",
    "model.add(TimeDistributed(Conv1D(32, kernel_size=3, activation='relu'), input_shape=(sequence_length, X.shape[2], 1)))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model.add(TimeDistributed(Dropout(0.2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.2)))\n",
    "model.add(LSTM(32, return_sequences=False, dropout=0.3, recurrent_dropout=0.2))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(len(actions), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d360c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = f'logs/cricket_model_{timestamp}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91942105",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "268242cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, update_freq='epoch'),\n",
    "    EarlyStopping(patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-7)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e696644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753699830.828332  492448 cuda_dnn.cc:529] Loaded cuDNN version 91100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 94ms/step - accuracy: 0.0682 - loss: 2.6312 - val_accuracy: 0.1675 - val_loss: 2.4869 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 76ms/step - accuracy: 0.1439 - loss: 2.4772 - val_accuracy: 0.1856 - val_loss: 2.3079 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 75ms/step - accuracy: 0.1870 - loss: 2.2657 - val_accuracy: 0.3093 - val_loss: 1.9944 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - accuracy: 0.2627 - loss: 2.0395 - val_accuracy: 0.3608 - val_loss: 1.7872 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 0.3323 - loss: 1.8402 - val_accuracy: 0.4407 - val_loss: 1.6022 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - accuracy: 0.4058 - loss: 1.6375 - val_accuracy: 0.4613 - val_loss: 1.4491 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - accuracy: 0.4512 - loss: 1.5883 - val_accuracy: 0.5232 - val_loss: 1.3638 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - accuracy: 0.5011 - loss: 1.4233 - val_accuracy: 0.5954 - val_loss: 1.1792 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - accuracy: 0.5390 - loss: 1.3028 - val_accuracy: 0.5902 - val_loss: 1.1695 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - accuracy: 0.5623 - loss: 1.2173 - val_accuracy: 0.6572 - val_loss: 0.9704 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.6450 - loss: 1.0602 - val_accuracy: 0.6856 - val_loss: 0.8638 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 0.6206 - loss: 1.0471 - val_accuracy: 0.7010 - val_loss: 0.8902 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 71ms/step - accuracy: 0.6520 - loss: 0.9681 - val_accuracy: 0.7216 - val_loss: 0.7815 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - accuracy: 0.7228 - loss: 0.8635 - val_accuracy: 0.6675 - val_loss: 0.8908 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - accuracy: 0.6822 - loss: 0.9249 - val_accuracy: 0.7500 - val_loss: 0.7615 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - accuracy: 0.7397 - loss: 0.7793 - val_accuracy: 0.7320 - val_loss: 0.7655 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 75ms/step - accuracy: 0.7237 - loss: 0.8233 - val_accuracy: 0.7629 - val_loss: 0.6418 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 76ms/step - accuracy: 0.7602 - loss: 0.7607 - val_accuracy: 0.7345 - val_loss: 0.7795 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - accuracy: 0.7568 - loss: 0.7267 - val_accuracy: 0.7990 - val_loss: 0.5846 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - accuracy: 0.8084 - loss: 0.6678 - val_accuracy: 0.8325 - val_loss: 0.5660 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.8000 - loss: 0.6606 - val_accuracy: 0.8119 - val_loss: 0.5237 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - accuracy: 0.8292 - loss: 0.5630 - val_accuracy: 0.7938 - val_loss: 0.5738 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.8264 - loss: 0.5568 - val_accuracy: 0.8247 - val_loss: 0.4884 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.8223 - loss: 0.5296 - val_accuracy: 0.8582 - val_loss: 0.4952 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 75ms/step - accuracy: 0.8403 - loss: 0.5237 - val_accuracy: 0.8814 - val_loss: 0.3829 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - accuracy: 0.8498 - loss: 0.4909 - val_accuracy: 0.8763 - val_loss: 0.3885 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - accuracy: 0.8645 - loss: 0.4115 - val_accuracy: 0.8840 - val_loss: 0.3428 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - accuracy: 0.8720 - loss: 0.4097 - val_accuracy: 0.8840 - val_loss: 0.3427 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - accuracy: 0.8704 - loss: 0.4311 - val_accuracy: 0.8376 - val_loss: 0.4742 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.8715 - loss: 0.4168 - val_accuracy: 0.9046 - val_loss: 0.3103 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.9106 - loss: 0.3037 - val_accuracy: 0.8943 - val_loss: 0.3219 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.8981 - loss: 0.3453 - val_accuracy: 0.8969 - val_loss: 0.3213 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.9111 - loss: 0.2919 - val_accuracy: 0.8969 - val_loss: 0.3081 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.9082 - loss: 0.3254 - val_accuracy: 0.8943 - val_loss: 0.3047 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.9060 - loss: 0.3084 - val_accuracy: 0.9021 - val_loss: 0.2978 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.9147 - loss: 0.3063 - val_accuracy: 0.9124 - val_loss: 0.2499 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - accuracy: 0.9224 - loss: 0.2585 - val_accuracy: 0.8943 - val_loss: 0.3201 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.9294 - loss: 0.2405 - val_accuracy: 0.9021 - val_loss: 0.2950 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - accuracy: 0.9145 - loss: 0.2879 - val_accuracy: 0.9330 - val_loss: 0.2454 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.9300 - loss: 0.2527 - val_accuracy: 0.9149 - val_loss: 0.2522 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - accuracy: 0.9275 - loss: 0.2362 - val_accuracy: 0.9072 - val_loss: 0.3083 - learning_rate: 0.0010\n",
      "Epoch 42/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.9404 - loss: 0.2330 - val_accuracy: 0.9278 - val_loss: 0.2190 - learning_rate: 0.0010\n",
      "Epoch 43/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.9401 - loss: 0.1875 - val_accuracy: 0.9201 - val_loss: 0.2731 - learning_rate: 0.0010\n",
      "Epoch 44/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - accuracy: 0.9536 - loss: 0.1673 - val_accuracy: 0.9433 - val_loss: 0.2037 - learning_rate: 0.0010\n",
      "Epoch 45/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - accuracy: 0.9502 - loss: 0.2003 - val_accuracy: 0.9253 - val_loss: 0.2649 - learning_rate: 0.0010\n",
      "Epoch 46/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - accuracy: 0.9323 - loss: 0.2199 - val_accuracy: 0.9175 - val_loss: 0.3087 - learning_rate: 0.0010\n",
      "Epoch 47/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.9259 - loss: 0.2227 - val_accuracy: 0.9253 - val_loss: 0.2672 - learning_rate: 0.0010\n",
      "Epoch 48/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - accuracy: 0.9600 - loss: 0.1354 - val_accuracy: 0.9278 - val_loss: 0.2591 - learning_rate: 0.0010\n",
      "Epoch 49/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - accuracy: 0.9439 - loss: 0.1735 - val_accuracy: 0.9407 - val_loss: 0.2413 - learning_rate: 0.0010\n",
      "Epoch 50/50\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - accuracy: 0.9515 - loss: 0.1816 - val_accuracy: 0.9098 - val_loss: 0.3129 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94eafadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9433\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76813ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('cricket_pose_mode_simple_even.h5')\n",
    "model.save('cricket_pose_model_simple_even.keras')\n",
    "np.save('cricket_label_map.npy', label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "353f781a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "I0000 00:00:1753700199.920077   15755 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1753700199.966096  531587 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 570.172.08), renderer: NVIDIA GeForce RTX 4070 SUPER/PCIe/SSE2\n",
      "W0000 00:00:1753700200.001929  531564 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753700200.022952  531575 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753700200.023877  531564 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753700200.025036  531560 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753700200.025105  531573 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753700200.029934  531578 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753700200.033732  531564 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753700200.034127  531567 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Load trained model and label map\n",
    "model = tf.keras.models.load_model('cricket_pose_mode_simple_even.h5')\n",
    "label_map = np.load('cricket_label_map.npy', allow_pickle=True).item()\n",
    "actions = list(label_map.keys())\n",
    "\n",
    "# Variables for prediction\n",
    "sequence = []\n",
    "sequence_length = 30\n",
    "threshold = 0.7\n",
    "\n",
    "# Start webcam\n",
    "cap = cv.VideoCapture('/home/smayan/Desktop/Cricket Pose Estimation /Model Training/WhatsApp Video 2025-07-27 at 16.12.07.mp4')\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Resize for consistent input\n",
    "        frame = cv.resize(frame, (640, 480))\n",
    "\n",
    "        # Detection\n",
    "        image = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = holistic.process(image)\n",
    "        image.flags.writeable = True\n",
    "        image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "\n",
    "        # Draw landmarks\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "            )\n",
    "\n",
    "        # Extract keypoints\n",
    "        if results.pose_landmarks:\n",
    "            keypoints = np.array([[res.x, res.y, res.z, res.visibility]\n",
    "                                  for res in results.pose_landmarks.landmark]).flatten()\n",
    "        else:\n",
    "            keypoints = np.zeros(33*4)\n",
    "\n",
    "        # Append to sequence\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-sequence_length:]\n",
    "\n",
    "        if len(sequence) == sequence_length:\n",
    "            input_seq = np.expand_dims(np.array(sequence), axis=0)\n",
    "            input_seq = input_seq.reshape(1, sequence_length, -1, 1)\n",
    "\n",
    "            # Predict\n",
    "            res = model.predict(input_seq, verbose=0)[0]\n",
    "            predicted_action = actions[np.argmax(res)]\n",
    "            confidence = np.max(res)\n",
    "\n",
    "            # Show prediction\n",
    "            if confidence > threshold:\n",
    "                cv.putText(image, f'{predicted_action}: {confidence:.2f}',\n",
    "                           (10, 50), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            # Show probabilities\n",
    "            for i, (action, prob) in enumerate(zip(actions, res)):\n",
    "                y_pos = 100 + i * 30\n",
    "                cv.rectangle(image, (10, y_pos), (int(prob * 300) + 10, y_pos + 25), (0, 255, 0), -1)\n",
    "                cv.putText(image, f'{action}: {prob:.2f}', (15, y_pos + 18),\n",
    "                           cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "        # Show output\n",
    "        cv.imshow('Cricket Pose Estimation', image)\n",
    "\n",
    "        # Quit\n",
    "        if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e492a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
