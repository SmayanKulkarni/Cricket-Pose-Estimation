{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05a62929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 16:25:54.957183: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-28 16:25:54.963992: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753700154.971881  646726 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753700154.974313  646726 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753700154.980511  646726 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753700154.980520  646726 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753700154.980521  646726 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753700154.980522  646726 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-28 16:25:54.982759: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94ae33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b6952f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/smayan/Desktop/Cricket Pose Estimation /Data'\n",
    "sequence_length = 30\n",
    "min_sequences_per_class = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac6830cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected cricket shots: ['Backfoot punch' 'Cover drive' 'Cut Shot' 'FBD' 'Flick'\n",
      " 'Front Foot defence' 'On Drive' 'Pull Shot' 'Reverse Sweep' 'Stance'\n",
      " 'Straight Drive' 'Sweep' 'Uppercut' 'loft']\n",
      "Backfoot punch: 19 videos\n",
      "Cover drive: 25 videos\n",
      "Cut Shot: 25 videos\n",
      "FBD: 15 videos\n",
      "Flick: 22 videos\n",
      "Front Foot defence: 25 videos\n",
      "On Drive: 25 videos\n",
      "Pull Shot: 25 videos\n",
      "Reverse Sweep: 25 videos\n",
      "Stance: 25 videos\n",
      "Straight Drive: 25 videos\n",
      "Sweep: 25 videos\n",
      "Uppercut: 25 videos\n",
      "loft: 25 videos\n"
     ]
    }
   ],
   "source": [
    "actions = np.array(sorted([folder for folder in os.listdir(DATA_PATH) \n",
    "                          if os.path.isdir(os.path.join(DATA_PATH, folder))]))\n",
    "print(f\"Detected cricket shots: {actions}\")\n",
    "for action in actions:\n",
    "    video_files = [f for f in os.listdir(os.path.join(DATA_PATH, action)) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "    video_files = video_files[:25]\n",
    "    print(f\"{action}: {len(video_files)} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f7ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "labels = []\n",
    "label_map = {label: num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f81cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1753713900.588326  187301 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1753713900.643152  187541 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 570.172.08), renderer: NVIDIA GeForce RTX 4070 SUPER/PCIe/SSE2\n",
      "Processing Backfoot punch:   0%|          | 0/19 [00:00<?, ?it/s]INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1753713900.682413  187517 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753713900.701276  187533 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753713900.703176  187516 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753713900.703198  187532 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753713900.704054  187523 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753713900.712489  187517 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753713900.714613  187531 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753713900.719205  187538 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753713900.732795  187519 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "Processing Backfoot punch: 100%|██████████| 19/19 [01:19<00:00,  4.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 112 sequences for Backfoot punch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Cover drive: 100%|██████████| 25/25 [01:35<00:00,  3.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 132 sequences for Cover drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Cut Shot: 100%|██████████| 25/25 [01:28<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 113 sequences for Cut Shot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing FBD: 100%|██████████| 15/15 [01:13<00:00,  4.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 91 sequences for FBD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Flick: 100%|██████████| 22/22 [-6:31:32<00:00, -0.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 112 sequences for Flick\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Front Foot defence: 100%|██████████| 25/25 [01:57<00:00,  4.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 150 sequences for Front Foot defence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing On Drive: 100%|██████████| 25/25 [01:09<00:00,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 94 sequences for On Drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Pull Shot: 100%|██████████| 25/25 [01:34<00:00,  3.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 131 sequences for Pull Shot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Reverse Sweep: 100%|██████████| 25/25 [01:59<00:00,  4.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 165 sequences for Reverse Sweep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Stance:  12%|█▏        | 3/25 [00:09<01:18,  3.56s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x636d78a39100] moov atom not found\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x636d7e567880] moov atom not found\n",
      "Processing Stance:  32%|███▏      | 8/25 [00:21<00:45,  2.67s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x636d82c60d40] moov atom not found\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x636d78348c80] moov atom not found\n",
      "Processing Stance:  64%|██████▍   | 16/25 [00:48<00:32,  3.56s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x636d7ca26c40] moov atom not found\n",
      "Processing Stance:  80%|████████  | 20/25 [01:06<00:21,  4.40s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x636d84495200] moov atom not found\n",
      "Processing Stance:  96%|█████████▌| 24/25 [01:18<00:03,  3.63s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x636d798be5c0] moov atom not found\n",
      "Processing Stance: 100%|██████████| 25/25 [01:18<00:00,  3.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 112 sequences for Stance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Straight Drive: 100%|██████████| 25/25 [02:18<00:00,  5.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 191 sequences for Straight Drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sweep: 100%|██████████| 25/25 [01:48<00:00,  4.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 149 sequences for Sweep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Uppercut: 100%|██████████| 25/25 [01:32<00:00,  3.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 126 sequences for Uppercut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing loft: 100%|██████████| 25/25 [01:49<00:00,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 149 sequences for loft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    for action in actions:\n",
    "        action_path = os.path.join(DATA_PATH, action)\n",
    "        video_files = [f for f in os.listdir(action_path) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "        video_files = video_files[:25]\n",
    "        action_sequences = []\n",
    "\n",
    "        for video_file in tqdm(video_files, desc=f\"Processing {action}\"):\n",
    "            video_path = os.path.join(action_path, video_file)\n",
    "            cap = cv.VideoCapture(video_path)\n",
    "            total_frames = int(cap.get(cv.CAP_PROP_FRAME_COUNT))\n",
    "            stride = max(1, sequence_length // 4)\n",
    "\n",
    "            for start_frame in tqdm(range(0, total_frames - sequence_length + 1, stride), \n",
    "                                    desc=f\"Frames in {video_file}\", leave=False):\n",
    "                cap.set(cv.CAP_PROP_POS_FRAMES, start_frame)\n",
    "                sequence = []\n",
    "\n",
    "                for _ in range(sequence_length):\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                    # frame = cv.resize(frame, (640, 480))\n",
    "                    image = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "                    image.flags.writeable = False\n",
    "                    results = holistic.process(image)\n",
    "                    image.flags.writeable = True\n",
    "                    image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "                    if results.pose_landmarks:\n",
    "                        pose = np.array([[res.x, res.y, res.z, res.visibility] \n",
    "                                         for res in results.pose_landmarks.landmark]).flatten()\n",
    "                    else:\n",
    "                        pose = np.zeros(33*4)\n",
    "                    sequence.append(pose)\n",
    "\n",
    "                if len(sequence) == sequence_length:\n",
    "                    action_sequences.append(sequence)\n",
    "\n",
    "            cap.release()\n",
    "\n",
    "        while len(action_sequences) < min_sequences_per_class:\n",
    "            if action_sequences:\n",
    "                original_seq = np.array(action_sequences[len(action_sequences) % len(action_sequences)])\n",
    "                noise = np.random.normal(0, 0.01, original_seq.shape)\n",
    "                augmented_seq = original_seq + noise\n",
    "                action_sequences.append(augmented_seq.tolist())\n",
    "\n",
    "        for seq in action_sequences:\n",
    "            sequences.append(seq)\n",
    "            labels.append(label_map[action])\n",
    "        print(f\"Generated {len(action_sequences)} sequences for {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "191ce2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1827, 30, 132)\n",
      "Labels shape: (1827,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(sequences)\n",
    "y = np.array(labels)\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "\n",
    "\n",
    "np.save('training_data_noresize.npy', X)\n",
    "np.save('labels.npy', y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9208210b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.5554406 ,  0.24139841, -0.18845586, ...,  0.77534622,\n",
       "          0.08676209,  0.38804743],\n",
       "        [ 0.55521965,  0.24002098, -0.25809762, ...,  0.7693364 ,\n",
       "          0.21733756,  0.39571512],\n",
       "        [ 0.55163604,  0.23883222, -0.26282305, ...,  0.76868081,\n",
       "          0.25477239,  0.40266779],\n",
       "        ...,\n",
       "        [ 0.37919977,  0.29017907,  0.15896487, ...,  0.72024399,\n",
       "          0.3753553 ,  0.7028569 ],\n",
       "        [ 0.37274122,  0.29221627,  0.21446192, ...,  0.72471052,\n",
       "          0.29324484,  0.72602713],\n",
       "        [ 0.36856019,  0.29323617,  0.14072758, ...,  0.72836959,\n",
       "          0.10429919,  0.74803698]],\n",
       "\n",
       "       [[ 0.50944591,  0.25721657, -0.21083091, ...,  0.76363158,\n",
       "          0.1139462 ,  0.71500087],\n",
       "        [ 0.52320641,  0.25505579, -0.24155426, ...,  0.77121109,\n",
       "          0.11704806,  0.68001729],\n",
       "        [ 0.51852798,  0.25573382, -0.22674216, ...,  0.77223337,\n",
       "          0.14097594,  0.6512875 ],\n",
       "        ...,\n",
       "        [ 0.34827006,  0.29428247,  0.15070571, ...,  0.73209471,\n",
       "         -0.26561132,  0.84881961],\n",
       "        [ 0.34717399,  0.29403761,  0.08361106, ...,  0.73071015,\n",
       "         -0.2880258 ,  0.84440207],\n",
       "        [ 0.34247047,  0.27182987,  0.17902488, ...,  0.72743881,\n",
       "         -0.25761878,  0.84032238]],\n",
       "\n",
       "       [[ 0.46119088,  0.26775602, -0.34944972, ...,  0.76438141,\n",
       "          0.09549927,  0.79977578],\n",
       "        [ 0.46975768,  0.26732156, -0.28787938, ...,  0.76601988,\n",
       "          0.18426935,  0.75819725],\n",
       "        [ 0.46042785,  0.26795703, -0.11246568, ...,  0.75917876,\n",
       "          0.22092137,  0.74011374],\n",
       "        ...,\n",
       "        [ 0.34906569,  0.2644906 ,  0.03881794, ...,  0.72469103,\n",
       "         -0.14383022,  0.85295111],\n",
       "        [ 0.34868261,  0.2656492 ,  0.00229136, ...,  0.72478062,\n",
       "         -0.09825981,  0.8487196 ],\n",
       "        [ 0.34778631,  0.26581252, -0.0065093 , ...,  0.72446036,\n",
       "         -0.09571968,  0.84646028]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.41325703,  0.1890786 , -0.1808124 , ...,  0.84397739,\n",
       "          0.3274419 ,  0.74332148],\n",
       "        [ 0.41286901,  0.20064361, -0.17787069, ...,  0.84680694,\n",
       "          0.22240485,  0.74955237],\n",
       "        [ 0.41232276,  0.1985914 , -0.15576217, ...,  0.8462159 ,\n",
       "          0.14289977,  0.75005841],\n",
       "        ...,\n",
       "        [ 0.43202677,  0.10250124, -0.171857  , ...,  0.87657064,\n",
       "         -0.16311722,  0.74794465],\n",
       "        [ 0.42943224,  0.10143775, -0.19244297, ...,  0.87659138,\n",
       "         -0.15979475,  0.74439585],\n",
       "        [ 0.42790341,  0.10110569, -0.21157819, ...,  0.87658101,\n",
       "         -0.17468312,  0.74519378]],\n",
       "\n",
       "       [[ 0.38855261,  0.17441459, -0.22123714, ...,  0.87199807,\n",
       "          0.09698085,  0.72984916],\n",
       "        [ 0.37399793,  0.17828743, -0.25825745, ...,  0.85410583,\n",
       "          0.21827859,  0.70169073],\n",
       "        [ 0.36673674,  0.17311949, -0.26217201, ...,  0.85722077,\n",
       "          0.06397771,  0.68321782],\n",
       "        ...,\n",
       "        [ 0.44784743,  0.09767452, -0.19296788, ...,  0.87692457,\n",
       "         -0.21121316,  0.7670278 ],\n",
       "        [ 0.45317891,  0.09736407, -0.19185957, ...,  0.87543255,\n",
       "         -0.13697369,  0.76021177],\n",
       "        [ 0.45885229,  0.09755029, -0.1449891 , ...,  0.875471  ,\n",
       "         -0.17986035,  0.7576201 ]],\n",
       "\n",
       "       [[ 0.38060215,  0.12118728, -0.34803388, ...,  0.87465268,\n",
       "         -0.18517356,  0.74403012],\n",
       "        [ 0.37014949,  0.12290786, -0.36867553, ...,  0.87451291,\n",
       "         -0.14505352,  0.73659492],\n",
       "        [ 0.37043723,  0.12141738, -0.35261196, ...,  0.87381482,\n",
       "         -0.10003949,  0.72956383],\n",
       "        ...,\n",
       "        [ 0.47088408,  0.09953246, -0.23520426, ...,  0.87722707,\n",
       "         -0.16220066,  0.74154997],\n",
       "        [ 0.47219941,  0.10036736, -0.23153716, ...,  0.876315  ,\n",
       "         -0.16953246,  0.74173707],\n",
       "        [ 0.47260204,  0.10194314, -0.25454128, ...,  0.87486804,\n",
       "         -0.16191772,  0.73782682]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d591a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "y_categorical = to_categorical(y, num_classes=len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "959e84bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_categorical, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb64909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a4b82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smayan/Desktop/AI-ML-DS/AI-and-ML-Course/.conda/lib/python3.11/site-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "I0000 00:00:1753695456.300549  187301 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 868 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# TimeDistributed CNN - 1 block only\n",
    "model.add(TimeDistributed(Conv1D(32, kernel_size=3, activation='relu'), input_shape=(sequence_length, X.shape[2], 1)))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model.add(TimeDistributed(Dropout(0.2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.2)))\n",
    "model.add(LSTM(32, return_sequences=False, dropout=0.3, recurrent_dropout=0.2))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(len(actions), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d360c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = f'logs/cricket_model_{timestamp}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91942105",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "268242cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, update_freq='epoch'),\n",
    "    EarlyStopping(patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-7)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e696644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753695462.066614  235584 cuda_dnn.cc:529] Loaded cuDNN version 91100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 80ms/step - accuracy: 0.0752 - loss: 2.6258 - val_accuracy: 0.1858 - val_loss: 2.3489 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - accuracy: 0.2032 - loss: 2.2698 - val_accuracy: 0.2842 - val_loss: 1.9890 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - accuracy: 0.2364 - loss: 2.0215 - val_accuracy: 0.4481 - val_loss: 1.5589 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 67ms/step - accuracy: 0.3746 - loss: 1.6785 - val_accuracy: 0.5000 - val_loss: 1.3193 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 66ms/step - accuracy: 0.4487 - loss: 1.4336 - val_accuracy: 0.5383 - val_loss: 1.1330 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - accuracy: 0.4582 - loss: 1.3500 - val_accuracy: 0.6612 - val_loss: 0.9405 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 65ms/step - accuracy: 0.5522 - loss: 1.1510 - val_accuracy: 0.7049 - val_loss: 0.8002 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 65ms/step - accuracy: 0.5965 - loss: 1.0264 - val_accuracy: 0.7350 - val_loss: 0.7562 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 65ms/step - accuracy: 0.6374 - loss: 0.9259 - val_accuracy: 0.7514 - val_loss: 0.6763 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - accuracy: 0.7046 - loss: 0.8184 - val_accuracy: 0.7678 - val_loss: 0.5770 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 66ms/step - accuracy: 0.7189 - loss: 0.7541 - val_accuracy: 0.8060 - val_loss: 0.5381 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 65ms/step - accuracy: 0.7591 - loss: 0.6626 - val_accuracy: 0.7842 - val_loss: 0.5809 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - accuracy: 0.7596 - loss: 0.6564 - val_accuracy: 0.8415 - val_loss: 0.4114 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - accuracy: 0.8085 - loss: 0.5875 - val_accuracy: 0.8306 - val_loss: 0.4253 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 72ms/step - accuracy: 0.8146 - loss: 0.5210 - val_accuracy: 0.8388 - val_loss: 0.4137 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 74ms/step - accuracy: 0.8260 - loss: 0.4875 - val_accuracy: 0.8743 - val_loss: 0.3590 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 77ms/step - accuracy: 0.8339 - loss: 0.5013 - val_accuracy: 0.8716 - val_loss: 0.3101 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 75ms/step - accuracy: 0.8443 - loss: 0.4862 - val_accuracy: 0.8825 - val_loss: 0.3089 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 74ms/step - accuracy: 0.8618 - loss: 0.3900 - val_accuracy: 0.8989 - val_loss: 0.2904 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 72ms/step - accuracy: 0.8903 - loss: 0.3340 - val_accuracy: 0.9208 - val_loss: 0.2395 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 75ms/step - accuracy: 0.9003 - loss: 0.3234 - val_accuracy: 0.9071 - val_loss: 0.2506 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 82ms/step - accuracy: 0.9156 - loss: 0.3134 - val_accuracy: 0.9153 - val_loss: 0.2305 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 79ms/step - accuracy: 0.9061 - loss: 0.2745 - val_accuracy: 0.9262 - val_loss: 0.2196 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 75ms/step - accuracy: 0.9349 - loss: 0.2387 - val_accuracy: 0.9180 - val_loss: 0.2431 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 74ms/step - accuracy: 0.9277 - loss: 0.2399 - val_accuracy: 0.9344 - val_loss: 0.1977 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 75ms/step - accuracy: 0.9095 - loss: 0.2905 - val_accuracy: 0.9454 - val_loss: 0.2043 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 74ms/step - accuracy: 0.9141 - loss: 0.2653 - val_accuracy: 0.9399 - val_loss: 0.1883 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 75ms/step - accuracy: 0.9539 - loss: 0.1763 - val_accuracy: 0.9372 - val_loss: 0.2055 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.9567 - loss: 0.1583 - val_accuracy: 0.9344 - val_loss: 0.1996 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.9342 - loss: 0.1974 - val_accuracy: 0.9153 - val_loss: 0.2141 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.9473 - loss: 0.1734 - val_accuracy: 0.9563 - val_loss: 0.1476 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.9504 - loss: 0.1729 - val_accuracy: 0.9536 - val_loss: 0.1753 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 74ms/step - accuracy: 0.9526 - loss: 0.1427 - val_accuracy: 0.9426 - val_loss: 0.1897 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 81ms/step - accuracy: 0.9619 - loss: 0.1212 - val_accuracy: 0.9590 - val_loss: 0.1673 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.9585 - loss: 0.1444 - val_accuracy: 0.9617 - val_loss: 0.1313 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 76ms/step - accuracy: 0.9525 - loss: 0.1367 - val_accuracy: 0.9508 - val_loss: 0.1713 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 74ms/step - accuracy: 0.9421 - loss: 0.2113 - val_accuracy: 0.9126 - val_loss: 0.3015 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.9269 - loss: 0.2548 - val_accuracy: 0.9536 - val_loss: 0.1440 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.9651 - loss: 0.1150 - val_accuracy: 0.9617 - val_loss: 0.1416 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.9624 - loss: 0.1118 - val_accuracy: 0.9454 - val_loss: 0.1511 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 74ms/step - accuracy: 0.9692 - loss: 0.1126 - val_accuracy: 0.9372 - val_loss: 0.2196 - learning_rate: 0.0010\n",
      "Epoch 42/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 74ms/step - accuracy: 0.9575 - loss: 0.1279 - val_accuracy: 0.9645 - val_loss: 0.1402 - learning_rate: 0.0010\n",
      "Epoch 43/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 72ms/step - accuracy: 0.9419 - loss: 0.1787 - val_accuracy: 0.9536 - val_loss: 0.1378 - learning_rate: 0.0010\n",
      "Epoch 44/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 75ms/step - accuracy: 0.9676 - loss: 0.0974 - val_accuracy: 0.9699 - val_loss: 0.1007 - learning_rate: 0.0010\n",
      "Epoch 45/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 75ms/step - accuracy: 0.9798 - loss: 0.0665 - val_accuracy: 0.9563 - val_loss: 0.1321 - learning_rate: 0.0010\n",
      "Epoch 46/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.9796 - loss: 0.0881 - val_accuracy: 0.9699 - val_loss: 0.1052 - learning_rate: 0.0010\n",
      "Epoch 47/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.9574 - loss: 0.1273 - val_accuracy: 0.9645 - val_loss: 0.1246 - learning_rate: 0.0010\n",
      "Epoch 48/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.9784 - loss: 0.0850 - val_accuracy: 0.9754 - val_loss: 0.0897 - learning_rate: 0.0010\n",
      "Epoch 49/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.9758 - loss: 0.0685 - val_accuracy: 0.9699 - val_loss: 0.1002 - learning_rate: 0.0010\n",
      "Epoch 50/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 74ms/step - accuracy: 0.9781 - loss: 0.0855 - val_accuracy: 0.9727 - val_loss: 0.1045 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94eafadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9754\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76813ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('best_cricket_pose_mode_simple_even.h5')\n",
    "model.save('best_cricket_pose_model_simple_even.keras')\n",
    "np.save('cricket_label_map.npy', label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "353f781a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "I0000 00:00:1753700435.711735  646726 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1753700435.756982  772300 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 570.172.08), renderer: NVIDIA GeForce RTX 4070 SUPER/PCIe/SSE2\n",
      "W0000 00:00:1753700435.789107  772275 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753700435.807220  772292 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753700435.808537  772272 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753700435.808544  772273 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753700435.808806  772292 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753700435.812893  772286 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753700435.817423  772280 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753700435.817515  772282 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Load trained model and label map\n",
    "model = tf.keras.models.load_model('cricket_pose_mode_simple_even.h5')\n",
    "label_map = np.load('cricket_label_map.npy', allow_pickle=True).item()\n",
    "actions = list(label_map.keys())\n",
    "\n",
    "# Variables for prediction\n",
    "sequence = []\n",
    "sequence_length = 30\n",
    "threshold = 0.9\n",
    "\n",
    "# Start webcam\n",
    "cap = cv.VideoCapture('/home/smayan/Desktop/Cricket Pose Estimation /Model Training/Test videos/test4.mp4')\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Resize for consistent input\n",
    "        # frame = cv.resize(frame, (640, 480))\n",
    "\n",
    "        # Detection\n",
    "        image = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = holistic.process(image)\n",
    "        image.flags.writeable = True\n",
    "        image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "\n",
    "        # Draw landmarks\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "            )\n",
    "\n",
    "        # Extract keypoints\n",
    "        if results.pose_landmarks:\n",
    "            keypoints = np.array([[res.x, res.y, res.z, res.visibility]\n",
    "                                  for res in results.pose_landmarks.landmark]).flatten()\n",
    "        else:\n",
    "            keypoints = np.zeros(33*4)\n",
    "\n",
    "        # Append to sequence\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-sequence_length:]\n",
    "\n",
    "        if len(sequence) == sequence_length:\n",
    "            input_seq = np.expand_dims(np.array(sequence), axis=0)\n",
    "            input_seq = input_seq.reshape(1, sequence_length, -1, 1)\n",
    "\n",
    "            # Predict\n",
    "            res = model.predict(input_seq, verbose=0)[0]\n",
    "            predicted_action = actions[np.argmax(res)]\n",
    "            confidence = np.max(res)\n",
    "\n",
    "            # Show prediction\n",
    "            if confidence > threshold:\n",
    "                cv.putText(image, f'{predicted_action}: {confidence:.2f}',\n",
    "                           (10, 50), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            # Show probabilities\n",
    "            for i, (action, prob) in enumerate(zip(actions, res)):\n",
    "                y_pos = 100 + i * 30\n",
    "                cv.rectangle(image, (10, y_pos), (int(prob * 300) + 10, y_pos + 25), (0, 255, 0), -1)\n",
    "                cv.putText(image, f'{action}: {prob:.2f}', (15, y_pos + 18),\n",
    "                           cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "        # Show output\n",
    "        cv.imshow('Cricket Pose Estimation', image)\n",
    "\n",
    "        # Quit\n",
    "        if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e492a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
