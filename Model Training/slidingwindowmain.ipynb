{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050d5942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64c195b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "933e4346",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/smayan/Desktop/Cricket Pose Estimation /Data'\n",
    "sequence_length = 30\n",
    "min_sequences_per_class = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b5dec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected cricket shots: ['Backfoot punch' 'Cover drive' 'Cut Shot' 'FBD' 'Flick'\n",
      " 'Front Food defence' 'On Drive' 'Pull Shot' 'Reverse Sweep' 'Stance'\n",
      " 'Straight Drive' 'Sweep' 'Uppercut' 'loft']\n",
      "Backfoot punch: 19 videos\n",
      "Cover drive: 29 videos\n",
      "Cut Shot: 43 videos\n",
      "FBD: 15 videos\n",
      "Flick: 22 videos\n",
      "Front Food defence: 32 videos\n",
      "On Drive: 40 videos\n",
      "Pull Shot: 40 videos\n",
      "Reverse Sweep: 30 videos\n",
      "Stance: 42 videos\n",
      "Straight Drive: 25 videos\n",
      "Sweep: 27 videos\n",
      "Uppercut: 29 videos\n",
      "loft: 31 videos\n"
     ]
    }
   ],
   "source": [
    "actions = np.array(sorted([folder for folder in os.listdir(DATA_PATH) \n",
    "                          if os.path.isdir(os.path.join(DATA_PATH, folder))]))\n",
    "print(f\"Detected cricket shots: {actions}\")\n",
    "for action in actions:\n",
    "    video_files = [f for f in os.listdir(os.path.join(DATA_PATH, action)) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "    print(f\"{action}: {len(video_files)} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08e8a122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753634054.267567    4932 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1753634054.337772    7482 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 570.172.08), renderer: NVIDIA GeForce RTX 4070 SUPER/PCIe/SSE2\n",
      "Processing Backfoot punch:   0%|          | 0/19 [00:00<?, ?it/s]W0000 00:00:1753634054.377479    7459 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753634054.401762    7471 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753634054.402800    7467 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753634054.403202    7475 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753634054.403298    7476 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753634054.407631    7455 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753634054.411351    7466 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753634054.412524    7481 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "Processing Backfoot punch: 100%|██████████| 19/19 [01:15<00:00,  3.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 112 sequences for Backfoot punch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Cover drive: 100%|██████████| 29/29 [01:49<00:00,  3.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 152 sequences for Cover drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Cut Shot: 100%|██████████| 43/43 [02:18<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 193 sequences for Cut Shot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing FBD: 100%|██████████| 15/15 [01:06<00:00,  4.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 91 sequences for FBD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Flick: 100%|██████████| 22/22 [01:22<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 112 sequences for Flick\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Front Food defence: 100%|██████████| 32/32 [02:16<00:00,  4.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 186 sequences for Front Food defence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing On Drive: 100%|██████████| 40/40 [01:52<00:00,  2.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 155 sequences for On Drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Pull Shot: 100%|██████████| 40/40 [02:32<00:00,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 212 sequences for Pull Shot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Reverse Sweep: 100%|██████████| 30/30 [02:25<00:00,  4.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 199 sequences for Reverse Sweep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Stance:   7%|▋         | 3/42 [00:09<02:22,  3.65s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x5b7c76ed9700] moov atom not found\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5b7c75d91d80] moov atom not found\n",
      "Processing Stance:  19%|█▉        | 8/42 [00:21<01:30,  2.65s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x5b7c7d231e80] moov atom not found\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5b7c752cca40] moov atom not found\n",
      "Processing Stance:  38%|███▊      | 16/42 [00:49<01:31,  3.53s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x5b7c754c2840] moov atom not found\n",
      "Processing Stance:  48%|████▊     | 20/42 [01:06<01:39,  4.51s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x5b7c74b87700] moov atom not found\n",
      "Processing Stance:  57%|█████▋    | 24/42 [01:19<01:08,  3.79s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x5b7c7ad7f1c0] moov atom not found\n",
      "Processing Stance:  71%|███████▏  | 30/42 [01:46<01:02,  5.17s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x5b7c7c6cf440] moov atom not found\n",
      "Processing Stance:  86%|████████▌ | 36/42 [02:06<00:25,  4.20s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x5b7c7e9f21c0] moov atom not found\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5b7c7d6e8ac0] moov atom not found\n",
      "Processing Stance:  93%|█████████▎| 39/42 [02:11<00:08,  2.71s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x5b7c75fa1000] moov atom not found\n",
      "Processing Stance: 100%|██████████| 42/42 [02:16<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 192 sequences for Stance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Straight Drive: 100%|██████████| 25/25 [02:17<00:00,  5.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 191 sequences for Straight Drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sweep: 100%|██████████| 27/27 [01:56<00:00,  4.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 159 sequences for Sweep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Uppercut: 100%|██████████| 29/29 [01:44<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 146 sequences for Uppercut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing loft: 100%|██████████| 31/31 [02:11<00:00,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 184 sequences for loft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    for action in actions:\n",
    "        action_path = os.path.join(DATA_PATH, action)\n",
    "        video_files = [f for f in os.listdir(action_path) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "        action_sequences = []\n",
    "\n",
    "        for video_file in tqdm(video_files, desc=f\"Processing {action}\"):\n",
    "            video_path = os.path.join(action_path, video_file)\n",
    "            cap = cv.VideoCapture(video_path)\n",
    "            total_frames = int(cap.get(cv.CAP_PROP_FRAME_COUNT))\n",
    "            stride = max(1, sequence_length // 4)\n",
    "\n",
    "            for start_frame in tqdm(range(0, total_frames - sequence_length + 1, stride), \n",
    "                                    desc=f\"Frames in {video_file}\", leave=False):\n",
    "                cap.set(cv.CAP_PROP_POS_FRAMES, start_frame)\n",
    "                sequence = []\n",
    "\n",
    "                for _ in range(sequence_length):\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                    frame = cv.resize(frame, (640, 480))\n",
    "                    image = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "                    image.flags.writeable = False\n",
    "                    results = holistic.process(image)\n",
    "                    image.flags.writeable = True\n",
    "                    image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "                    if results.pose_landmarks:\n",
    "                        pose = np.array([[res.x, res.y, res.z, res.visibility] \n",
    "                                         for res in results.pose_landmarks.landmark]).flatten()\n",
    "                    else:\n",
    "                        pose = np.zeros(33*4)\n",
    "                    sequence.append(pose)\n",
    "\n",
    "                if len(sequence) == sequence_length:\n",
    "                    action_sequences.append(sequence)\n",
    "\n",
    "            cap.release()\n",
    "\n",
    "        while len(action_sequences) < min_sequences_per_class:\n",
    "            if action_sequences:\n",
    "                original_seq = np.array(action_sequences[len(action_sequences) % len(action_sequences)])\n",
    "                noise = np.random.normal(0, 0.01, original_seq.shape)\n",
    "                augmented_seq = original_seq + noise\n",
    "                action_sequences.append(augmented_seq.tolist())\n",
    "\n",
    "        for seq in action_sequences:\n",
    "            sequences.append(seq)\n",
    "            labels.append(label_map[action])\n",
    "        print(f\"Generated {len(action_sequences)} sequences for {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72cbcded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (2284, 30, 132)\n",
      "Labels shape: (2284,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(sequences)\n",
    "y = np.array(labels)\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eabdea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "y_categorical = to_categorical(y, num_classes=len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11d3d8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_categorical, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52786a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9d24f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# TimeDistributed CNN - 1 block only\n",
    "model.add(TimeDistributed(Conv1D(32, kernel_size=3, activation='relu'), input_shape=(sequence_length, X.shape[2], 1)))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model.add(TimeDistributed(Dropout(0.2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.2)))\n",
    "model.add(LSTM(32, return_sequences=False, dropout=0.3, recurrent_dropout=0.2))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(len(actions), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bfab4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = f'logs/cricket_model_{timestamp}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "deb157b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2becc92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, update_freq='epoch'),\n",
    "    EarlyStopping(patience=15, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-7)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c94d14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753635786.227966   20763 cuda_dnn.cc:529] Loaded cuDNN version 91100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - accuracy: 0.0937 - loss: 2.6333 - val_accuracy: 0.1969 - val_loss: 2.2696 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - accuracy: 0.1625 - loss: 2.3372 - val_accuracy: 0.3282 - val_loss: 1.9912 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - accuracy: 0.2492 - loss: 2.0888 - val_accuracy: 0.3961 - val_loss: 1.7002 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.3358 - loss: 1.8538 - val_accuracy: 0.4639 - val_loss: 1.5498 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.3994 - loss: 1.6935 - val_accuracy: 0.5164 - val_loss: 1.3462 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.4793 - loss: 1.5020 - val_accuracy: 0.5339 - val_loss: 1.2812 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.4835 - loss: 1.4736 - val_accuracy: 0.5996 - val_loss: 1.1287 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.5613 - loss: 1.2801 - val_accuracy: 0.6324 - val_loss: 1.0816 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - accuracy: 0.5771 - loss: 1.2271 - val_accuracy: 0.6543 - val_loss: 0.9508 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.6184 - loss: 1.0965 - val_accuracy: 0.6608 - val_loss: 0.8825 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.6361 - loss: 1.0391 - val_accuracy: 0.6521 - val_loss: 0.8967 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.6824 - loss: 0.9322 - val_accuracy: 0.7440 - val_loss: 0.7245 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.6845 - loss: 0.9170 - val_accuracy: 0.7177 - val_loss: 0.8025 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.7290 - loss: 0.8473 - val_accuracy: 0.7724 - val_loss: 0.6835 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.7694 - loss: 0.7246 - val_accuracy: 0.7877 - val_loss: 0.5922 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.7572 - loss: 0.7185 - val_accuracy: 0.8118 - val_loss: 0.5926 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - accuracy: 0.7845 - loss: 0.6846 - val_accuracy: 0.8074 - val_loss: 0.5726 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.7903 - loss: 0.6423 - val_accuracy: 0.8249 - val_loss: 0.5577 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.8217 - loss: 0.5641 - val_accuracy: 0.8293 - val_loss: 0.5081 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.8086 - loss: 0.5677 - val_accuracy: 0.8359 - val_loss: 0.5102 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - accuracy: 0.8118 - loss: 0.5235 - val_accuracy: 0.8731 - val_loss: 0.4521 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.8350 - loss: 0.5077 - val_accuracy: 0.8425 - val_loss: 0.4963 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.8191 - loss: 0.5302 - val_accuracy: 0.8621 - val_loss: 0.4236 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.8530 - loss: 0.4638 - val_accuracy: 0.8621 - val_loss: 0.4144 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.8562 - loss: 0.5102 - val_accuracy: 0.8709 - val_loss: 0.4227 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.8395 - loss: 0.4986 - val_accuracy: 0.8512 - val_loss: 0.4712 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.8874 - loss: 0.3945 - val_accuracy: 0.8731 - val_loss: 0.4197 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - accuracy: 0.8726 - loss: 0.4191 - val_accuracy: 0.8578 - val_loss: 0.4744 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.8485 - loss: 0.5129 - val_accuracy: 0.8796 - val_loss: 0.4378 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.8762 - loss: 0.4035 - val_accuracy: 0.8775 - val_loss: 0.4429 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9035 - loss: 0.3322 - val_accuracy: 0.8972 - val_loss: 0.3398 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.8780 - loss: 0.4149 - val_accuracy: 0.8950 - val_loss: 0.3691 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.8831 - loss: 0.3705 - val_accuracy: 0.9015 - val_loss: 0.3505 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9373 - loss: 0.2448 - val_accuracy: 0.9081 - val_loss: 0.3336 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9265 - loss: 0.2805 - val_accuracy: 0.9059 - val_loss: 0.3364 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9253 - loss: 0.2478 - val_accuracy: 0.9103 - val_loss: 0.3142 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9025 - loss: 0.3476 - val_accuracy: 0.9234 - val_loss: 0.3094 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9450 - loss: 0.1997 - val_accuracy: 0.9125 - val_loss: 0.3269 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9312 - loss: 0.2344 - val_accuracy: 0.8840 - val_loss: 0.4352 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9220 - loss: 0.2604 - val_accuracy: 0.9190 - val_loss: 0.3113 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9480 - loss: 0.1907 - val_accuracy: 0.9059 - val_loss: 0.3580 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9419 - loss: 0.2212 - val_accuracy: 0.9190 - val_loss: 0.3452 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9411 - loss: 0.1837 - val_accuracy: 0.8993 - val_loss: 0.3656 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9279 - loss: 0.2294 - val_accuracy: 0.9168 - val_loss: 0.2968 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9570 - loss: 0.1643 - val_accuracy: 0.9278 - val_loss: 0.2848 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9329 - loss: 0.2043 - val_accuracy: 0.9256 - val_loss: 0.3094 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9480 - loss: 0.1789 - val_accuracy: 0.9322 - val_loss: 0.2952 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9301 - loss: 0.2290 - val_accuracy: 0.9322 - val_loss: 0.3063 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9155 - loss: 0.2921 - val_accuracy: 0.9015 - val_loss: 0.4014 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.8998 - loss: 0.3454 - val_accuracy: 0.9256 - val_loss: 0.3570 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9422 - loss: 0.2213 - val_accuracy: 0.9431 - val_loss: 0.2612 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9496 - loss: 0.1918 - val_accuracy: 0.8884 - val_loss: 0.4746 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9196 - loss: 0.2656 - val_accuracy: 0.9234 - val_loss: 0.2849 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9579 - loss: 0.1646 - val_accuracy: 0.9234 - val_loss: 0.3424 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9627 - loss: 0.1460 - val_accuracy: 0.9322 - val_loss: 0.3021 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9514 - loss: 0.1675 - val_accuracy: 0.9212 - val_loss: 0.3071 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9557 - loss: 0.1212 - val_accuracy: 0.9322 - val_loss: 0.3083 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9642 - loss: 0.1104 - val_accuracy: 0.9365 - val_loss: 0.2993 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9661 - loss: 0.0977 - val_accuracy: 0.9234 - val_loss: 0.3423 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9475 - loss: 0.1907 - val_accuracy: 0.9409 - val_loss: 0.2421 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9669 - loss: 0.1033 - val_accuracy: 0.9365 - val_loss: 0.2619 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9730 - loss: 0.0892 - val_accuracy: 0.9300 - val_loss: 0.2842 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9413 - loss: 0.1891 - val_accuracy: 0.9081 - val_loss: 0.4350 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9620 - loss: 0.1740 - val_accuracy: 0.9344 - val_loss: 0.2929 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9568 - loss: 0.1427 - val_accuracy: 0.9497 - val_loss: 0.2528 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9702 - loss: 0.0932 - val_accuracy: 0.9540 - val_loss: 0.2286 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9733 - loss: 0.1028 - val_accuracy: 0.9431 - val_loss: 0.2868 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9767 - loss: 0.0869 - val_accuracy: 0.8950 - val_loss: 0.5109 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9597 - loss: 0.1287 - val_accuracy: 0.9453 - val_loss: 0.2599 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9651 - loss: 0.1477 - val_accuracy: 0.9322 - val_loss: 0.2938 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9544 - loss: 0.1506 - val_accuracy: 0.9475 - val_loss: 0.2622 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9740 - loss: 0.0895 - val_accuracy: 0.9562 - val_loss: 0.2274 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9646 - loss: 0.1103 - val_accuracy: 0.9475 - val_loss: 0.2634 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9302 - loss: 0.2410 - val_accuracy: 0.9475 - val_loss: 0.2886 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9788 - loss: 0.0771 - val_accuracy: 0.9497 - val_loss: 0.2414 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9717 - loss: 0.0916 - val_accuracy: 0.9278 - val_loss: 0.3375 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9716 - loss: 0.1251 - val_accuracy: 0.9497 - val_loss: 0.2669 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9789 - loss: 0.0771 - val_accuracy: 0.9322 - val_loss: 0.3240 - learning_rate: 0.0010\n",
      "Epoch 79/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9760 - loss: 0.0765 - val_accuracy: 0.9409 - val_loss: 0.2795 - learning_rate: 0.0010\n",
      "Epoch 80/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9594 - loss: 0.1590 - val_accuracy: 0.9234 - val_loss: 0.3394 - learning_rate: 0.0010\n",
      "Epoch 81/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9673 - loss: 0.1115 - val_accuracy: 0.9475 - val_loss: 0.2567 - learning_rate: 0.0010\n",
      "Epoch 82/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9768 - loss: 0.0953 - val_accuracy: 0.9497 - val_loss: 0.2781 - learning_rate: 0.0010\n",
      "Epoch 83/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9858 - loss: 0.0567 - val_accuracy: 0.9584 - val_loss: 0.2420 - learning_rate: 5.0000e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9770 - loss: 0.0934 - val_accuracy: 0.9606 - val_loss: 0.2371 - learning_rate: 5.0000e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9900 - loss: 0.0405 - val_accuracy: 0.9562 - val_loss: 0.2358 - learning_rate: 5.0000e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9826 - loss: 0.0671 - val_accuracy: 0.9584 - val_loss: 0.2341 - learning_rate: 5.0000e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9847 - loss: 0.0517 - val_accuracy: 0.9540 - val_loss: 0.2383 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0795eecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9562\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30ab1f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('cricket_pose_mode_simple.h5')\n",
    "model.save('cricket_pose_model_simple.keras')\n",
    "np.save('cricket_label_map.npy', label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e220a528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "I0000 00:00:1753636258.364997    4932 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1753636258.396638   34415 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 570.172.08), renderer: NVIDIA GeForce RTX 4070 SUPER/PCIe/SSE2\n",
      "W0000 00:00:1753636258.425987   34394 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753636258.443579   34387 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753636258.444953   34413 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753636258.445092   34387 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753636258.445313   34405 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753636258.449397   34406 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753636258.454228   34387 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753636258.455119   34399 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Load trained model and label map\n",
    "model = tf.keras.models.load_model('cricket_pose_model.h5')\n",
    "label_map = np.load('cricket_label_map.npy', allow_pickle=True).item()\n",
    "actions = list(label_map.keys())\n",
    "\n",
    "# Variables for prediction\n",
    "sequence = []\n",
    "sequence_length = 30\n",
    "threshold = 0.7\n",
    "\n",
    "# Start webcam\n",
    "cap = cv.VideoCapture('/home/smayan/Desktop/Cricket Pose Estimation /Model Training/WhatsApp Video 2025-07-27 at 16.12.07.mp4')\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Resize for consistent input\n",
    "        frame = cv.resize(frame, (640, 480))\n",
    "\n",
    "        # Detection\n",
    "        image = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = holistic.process(image)\n",
    "        image.flags.writeable = True\n",
    "        image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "\n",
    "        # Draw landmarks\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "            )\n",
    "\n",
    "        # Extract keypoints\n",
    "        if results.pose_landmarks:\n",
    "            keypoints = np.array([[res.x, res.y, res.z, res.visibility]\n",
    "                                  for res in results.pose_landmarks.landmark]).flatten()\n",
    "        else:\n",
    "            keypoints = np.zeros(33*4)\n",
    "\n",
    "        # Append to sequence\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-sequence_length:]\n",
    "\n",
    "        if len(sequence) == sequence_length:\n",
    "            input_seq = np.expand_dims(np.array(sequence), axis=0)\n",
    "            input_seq = input_seq.reshape(1, sequence_length, -1, 1)\n",
    "\n",
    "            # Predict\n",
    "            res = model.predict(input_seq, verbose=0)[0]\n",
    "            predicted_action = actions[np.argmax(res)]\n",
    "            confidence = np.max(res)\n",
    "\n",
    "            # Show prediction\n",
    "            if confidence > threshold:\n",
    "                cv.putText(image, f'{predicted_action}: {confidence:.2f}',\n",
    "                           (10, 50), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            # Show probabilities\n",
    "            for i, (action, prob) in enumerate(zip(actions, res)):\n",
    "                y_pos = 100 + i * 30\n",
    "                cv.rectangle(image, (10, y_pos), (int(prob * 300) + 10, y_pos + 25), (0, 255, 0), -1)\n",
    "                cv.putText(image, f'{action}: {prob:.2f}', (15, y_pos + 18),\n",
    "                           cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "        # Show output\n",
    "        cv.imshow('Cricket Pose Estimation', image)\n",
    "\n",
    "        # Quit\n",
    "        if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47fef18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
