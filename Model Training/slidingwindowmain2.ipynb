{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "050d5942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten, TimeDistributed\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64c195b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "933e4346",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/smayan/Desktop/Cricket Pose Estimation /Data'\n",
    "sequence_length = 30\n",
    "min_sequences_per_class = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6b5dec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected cricket shots: ['Backfoot punch' 'Cover drive' 'Cut Shot' 'FBD' 'Flick'\n",
      " 'Front Food defence' 'On Drive' 'Pull Shot' 'Reverse Sweep'\n",
      " 'Straight Drive' 'Sweep' 'Uppercut' 'loft']\n",
      "Backfoot punch: 19 videos\n",
      "Cover drive: 29 videos\n",
      "Cut Shot: 43 videos\n",
      "FBD: 15 videos\n",
      "Flick: 22 videos\n",
      "Front Food defence: 32 videos\n",
      "On Drive: 40 videos\n",
      "Pull Shot: 40 videos\n",
      "Reverse Sweep: 30 videos\n",
      "Straight Drive: 25 videos\n",
      "Sweep: 27 videos\n",
      "Uppercut: 29 videos\n",
      "loft: 31 videos\n"
     ]
    }
   ],
   "source": [
    "actions = np.array(sorted([folder for folder in os.listdir(DATA_PATH) \n",
    "                          if os.path.isdir(os.path.join(DATA_PATH, folder))]))\n",
    "print(f\"Detected cricket shots: {actions}\")\n",
    "for action in actions:\n",
    "    video_files = [f for f in os.listdir(os.path.join(DATA_PATH, action)) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "    print(f\"{action}: {len(video_files)} videos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a383821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "labels = []\n",
    "label_map = {label: num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08e8a122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1753630544.439201   34338 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1753630544.492223   34584 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 570.172.08), renderer: NVIDIA GeForce RTX 4070 SUPER/PCIe/SSE2\n",
      "Processing Backfoot punch:   0%|          | 0/19 [00:00<?, ?it/s]INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1753630544.531157   34569 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753630544.549785   34580 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753630544.551244   34570 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753630544.551617   34581 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753630544.552225   34573 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753630544.556879   34580 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753630544.561077   34578 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753630544.561420   34572 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753630544.575265   34557 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "Processing Backfoot punch: 100%|██████████| 19/19 [01:18<00:00,  4.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 112 sequences for Backfoot punch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Cover drive: 100%|██████████| 29/29 [01:54<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 152 sequences for Cover drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Cut Shot: 100%|██████████| 43/43 [02:24<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 193 sequences for Cut Shot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing FBD: 100%|██████████| 15/15 [01:08<00:00,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 91 sequences for FBD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Flick: 100%|██████████| 22/22 [01:25<00:00,  3.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 112 sequences for Flick\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Front Food defence: 100%|██████████| 32/32 [02:22<00:00,  4.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 186 sequences for Front Food defence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing On Drive: 100%|██████████| 40/40 [01:57<00:00,  2.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 155 sequences for On Drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Pull Shot: 100%|██████████| 40/40 [02:38<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 212 sequences for Pull Shot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Reverse Sweep: 100%|██████████| 30/30 [02:32<00:00,  5.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 199 sequences for Reverse Sweep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Straight Drive: 100%|██████████| 25/25 [02:23<00:00,  5.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 191 sequences for Straight Drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sweep: 100%|██████████| 27/27 [02:00<00:00,  4.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 159 sequences for Sweep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Uppercut: 100%|██████████| 29/29 [01:49<00:00,  3.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 146 sequences for Uppercut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing loft: 100%|██████████| 31/31 [02:17<00:00,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 184 sequences for loft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    for action in actions:\n",
    "        action_path = os.path.join(DATA_PATH, action)\n",
    "        video_files = [f for f in os.listdir(action_path) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "        action_sequences = []\n",
    "\n",
    "        for video_file in tqdm(video_files, desc=f\"Processing {action}\"):\n",
    "            video_path = os.path.join(action_path, video_file)\n",
    "            cap = cv.VideoCapture(video_path)\n",
    "            total_frames = int(cap.get(cv.CAP_PROP_FRAME_COUNT))\n",
    "            stride = max(1, sequence_length // 4)\n",
    "\n",
    "            for start_frame in tqdm(range(0, total_frames - sequence_length + 1, stride), \n",
    "                                    desc=f\"Frames in {video_file}\", leave=False):\n",
    "                cap.set(cv.CAP_PROP_POS_FRAMES, start_frame)\n",
    "                sequence = []\n",
    "\n",
    "                for _ in range(sequence_length):\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                    frame = cv.resize(frame, (640, 480))\n",
    "                    image = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "                    image.flags.writeable = False\n",
    "                    results = holistic.process(image)\n",
    "                    image.flags.writeable = True\n",
    "                    image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "                    if results.pose_landmarks:\n",
    "                        pose = np.array([[res.x, res.y, res.z, res.visibility] \n",
    "                                         for res in results.pose_landmarks.landmark]).flatten()\n",
    "                    else:\n",
    "                        pose = np.zeros(33*4)\n",
    "                    sequence.append(pose)\n",
    "\n",
    "                if len(sequence) == sequence_length:\n",
    "                    action_sequences.append(sequence)\n",
    "\n",
    "            cap.release()\n",
    "\n",
    "        while len(action_sequences) < min_sequences_per_class:\n",
    "            if action_sequences:\n",
    "                original_seq = np.array(action_sequences[len(action_sequences) % len(action_sequences)])\n",
    "                noise = np.random.normal(0, 0.01, original_seq.shape)\n",
    "                augmented_seq = original_seq + noise\n",
    "                action_sequences.append(augmented_seq.tolist())\n",
    "\n",
    "        for seq in action_sequences:\n",
    "            sequences.append(seq)\n",
    "            labels.append(label_map[action])\n",
    "        print(f\"Generated {len(action_sequences)} sequences for {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72cbcded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (2092, 30, 132)\n",
      "Labels shape: (2092,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(sequences)\n",
    "y = np.array(labels)\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eabdea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "y_categorical = to_categorical(y, num_classes=len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11d3d8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_categorical, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52786a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a9d24f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smayan/Desktop/AI-ML-DS/AI-and-ML-Course/.conda/lib/python3.11/site-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "I0000 00:00:1753632119.587438   34338 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1154 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv1D(64, kernel_size=3, activation='relu'), input_shape=(sequence_length, X.shape[2], 1)))\n",
    "model.add(TimeDistributed(Conv1D(64, kernel_size=3, activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "model.add(TimeDistributed(Conv1D(128, kernel_size=3, activation='relu')))\n",
    "model.add(TimeDistributed(Conv1D(128, kernel_size=3, activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(LSTM(64, return_sequences=False, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(actions), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bfab4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = f'logs/cricket_model_{timestamp}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deb157b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2becc92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, update_freq='epoch'),\n",
    "    EarlyStopping(patience=15, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-7)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c94d14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753632128.534471   47413 cuda_dnn.cc:529] Loaded cuDNN version 91100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 85ms/step - accuracy: 0.0784 - loss: 2.6215 - val_accuracy: 0.0979 - val_loss: 2.5623 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - accuracy: 0.0857 - loss: 2.5333 - val_accuracy: 0.1575 - val_loss: 2.3869 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.1820 - loss: 2.3239 - val_accuracy: 0.2267 - val_loss: 2.1264 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.2062 - loss: 2.1847 - val_accuracy: 0.2912 - val_loss: 2.0094 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.1849 - loss: 2.1185 - val_accuracy: 0.3270 - val_loss: 1.8970 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.2398 - loss: 2.0281 - val_accuracy: 0.3103 - val_loss: 1.9280 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.2499 - loss: 1.9890 - val_accuracy: 0.3604 - val_loss: 1.7439 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.2897 - loss: 1.9493 - val_accuracy: 0.4129 - val_loss: 1.7084 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.3596 - loss: 1.7667 - val_accuracy: 0.4081 - val_loss: 1.6308 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - accuracy: 0.3508 - loss: 1.7522 - val_accuracy: 0.4248 - val_loss: 1.5536 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.3585 - loss: 1.6913 - val_accuracy: 0.4272 - val_loss: 1.6239 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - accuracy: 0.3996 - loss: 1.6545 - val_accuracy: 0.4821 - val_loss: 1.4601 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.4524 - loss: 1.4891 - val_accuracy: 0.4749 - val_loss: 1.4258 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.4504 - loss: 1.4887 - val_accuracy: 0.5251 - val_loss: 1.3248 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.5020 - loss: 1.3481 - val_accuracy: 0.4916 - val_loss: 1.3829 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - accuracy: 0.5086 - loss: 1.2841 - val_accuracy: 0.5274 - val_loss: 1.2583 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.5891 - loss: 1.1747 - val_accuracy: 0.5752 - val_loss: 1.1927 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - accuracy: 0.5741 - loss: 1.1948 - val_accuracy: 0.5513 - val_loss: 1.2433 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.5955 - loss: 1.1182 - val_accuracy: 0.5895 - val_loss: 1.1104 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - accuracy: 0.6085 - loss: 1.1322 - val_accuracy: 0.5967 - val_loss: 1.1393 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.6113 - loss: 1.0443 - val_accuracy: 0.6897 - val_loss: 0.8440 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - accuracy: 0.6946 - loss: 0.8926 - val_accuracy: 0.5919 - val_loss: 1.2499 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.6634 - loss: 0.9588 - val_accuracy: 0.6396 - val_loss: 1.1132 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.7101 - loss: 0.8761 - val_accuracy: 0.7017 - val_loss: 0.8394 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.7332 - loss: 0.8265 - val_accuracy: 0.6778 - val_loss: 0.9783 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - accuracy: 0.7500 - loss: 0.7376 - val_accuracy: 0.7375 - val_loss: 0.7577 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - accuracy: 0.7187 - loss: 0.8153 - val_accuracy: 0.7613 - val_loss: 0.7067 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.7721 - loss: 0.6576 - val_accuracy: 0.7446 - val_loss: 0.8567 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.7954 - loss: 0.6730 - val_accuracy: 0.6802 - val_loss: 1.0963 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.7311 - loss: 0.7785 - val_accuracy: 0.8019 - val_loss: 0.6323 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.8122 - loss: 0.5817 - val_accuracy: 0.7924 - val_loss: 0.6775 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - accuracy: 0.8270 - loss: 0.5516 - val_accuracy: 0.7900 - val_loss: 0.6741 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.8534 - loss: 0.4757 - val_accuracy: 0.8067 - val_loss: 0.7074 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - accuracy: 0.8284 - loss: 0.5751 - val_accuracy: 0.8162 - val_loss: 0.6250 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 69ms/step - accuracy: 0.8303 - loss: 0.5772 - val_accuracy: 0.7733 - val_loss: 0.7048 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.7930 - loss: 0.6858 - val_accuracy: 0.7804 - val_loss: 0.7197 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.8495 - loss: 0.4894 - val_accuracy: 0.6754 - val_loss: 1.2706 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - accuracy: 0.7770 - loss: 0.7938 - val_accuracy: 0.7804 - val_loss: 0.7389 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.8364 - loss: 0.5227 - val_accuracy: 0.8258 - val_loss: 0.6088 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.8967 - loss: 0.3496 - val_accuracy: 0.8401 - val_loss: 0.5385 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 69ms/step - accuracy: 0.8932 - loss: 0.3360 - val_accuracy: 0.8425 - val_loss: 0.5080 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.8537 - loss: 0.5485 - val_accuracy: 0.8115 - val_loss: 0.7073 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - accuracy: 0.8500 - loss: 0.5046 - val_accuracy: 0.8496 - val_loss: 0.5324 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.8990 - loss: 0.3690 - val_accuracy: 0.8568 - val_loss: 0.4859 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.9056 - loss: 0.3063 - val_accuracy: 0.8831 - val_loss: 0.4896 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - accuracy: 0.9136 - loss: 0.3395 - val_accuracy: 0.8687 - val_loss: 0.5042 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.9189 - loss: 0.2932 - val_accuracy: 0.8663 - val_loss: 0.4784 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 69ms/step - accuracy: 0.9144 - loss: 0.3060 - val_accuracy: 0.8711 - val_loss: 0.5159 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 69ms/step - accuracy: 0.9054 - loss: 0.2819 - val_accuracy: 0.8854 - val_loss: 0.4977 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.9315 - loss: 0.3091 - val_accuracy: 0.8616 - val_loss: 0.5770 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - accuracy: 0.9482 - loss: 0.2313 - val_accuracy: 0.8950 - val_loss: 0.4377 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - accuracy: 0.9468 - loss: 0.2263 - val_accuracy: 0.8902 - val_loss: 0.4403 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 77ms/step - accuracy: 0.9541 - loss: 0.1720 - val_accuracy: 0.9045 - val_loss: 0.4414 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - accuracy: 0.9544 - loss: 0.1669 - val_accuracy: 0.8926 - val_loss: 0.5587 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - accuracy: 0.9455 - loss: 0.2030 - val_accuracy: 0.9165 - val_loss: 0.4029 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - accuracy: 0.9131 - loss: 0.3367 - val_accuracy: 0.8735 - val_loss: 0.5620 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - accuracy: 0.9113 - loss: 0.3620 - val_accuracy: 0.8019 - val_loss: 0.8194 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - accuracy: 0.9093 - loss: 0.3122 - val_accuracy: 0.8926 - val_loss: 0.3824 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - accuracy: 0.9589 - loss: 0.1526 - val_accuracy: 0.9045 - val_loss: 0.4140 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - accuracy: 0.9567 - loss: 0.1755 - val_accuracy: 0.9141 - val_loss: 0.3804 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 72ms/step - accuracy: 0.9493 - loss: 0.1499 - val_accuracy: 0.9093 - val_loss: 0.4371 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 72ms/step - accuracy: 0.9521 - loss: 0.1628 - val_accuracy: 0.9117 - val_loss: 0.4020 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - accuracy: 0.9663 - loss: 0.1312 - val_accuracy: 0.8807 - val_loss: 0.5636 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - accuracy: 0.9465 - loss: 0.1808 - val_accuracy: 0.9165 - val_loss: 0.3699 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - accuracy: 0.9582 - loss: 0.1450 - val_accuracy: 0.9284 - val_loss: 0.3284 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - accuracy: 0.9659 - loss: 0.1209 - val_accuracy: 0.9165 - val_loss: 0.3884 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - accuracy: 0.9642 - loss: 0.1294 - val_accuracy: 0.9093 - val_loss: 0.4522 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - accuracy: 0.9646 - loss: 0.1019 - val_accuracy: 0.9117 - val_loss: 0.4041 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - accuracy: 0.9639 - loss: 0.1656 - val_accuracy: 0.9189 - val_loss: 0.4507 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - accuracy: 0.9421 - loss: 0.2183 - val_accuracy: 0.8998 - val_loss: 0.3984 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - accuracy: 0.9471 - loss: 0.1980 - val_accuracy: 0.9069 - val_loss: 0.4159 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - accuracy: 0.9266 - loss: 0.3073 - val_accuracy: 0.9475 - val_loss: 0.2582 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - accuracy: 0.9677 - loss: 0.1184 - val_accuracy: 0.9332 - val_loss: 0.2950 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - accuracy: 0.9696 - loss: 0.1143 - val_accuracy: 0.9260 - val_loss: 0.3981 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - accuracy: 0.9773 - loss: 0.0964 - val_accuracy: 0.8974 - val_loss: 0.5548 - learning_rate: 0.0010\n",
      "Epoch 76/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - accuracy: 0.9538 - loss: 0.1879 - val_accuracy: 0.8807 - val_loss: 0.5636 - learning_rate: 0.0010\n",
      "Epoch 77/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - accuracy: 0.9586 - loss: 0.1415 - val_accuracy: 0.9332 - val_loss: 0.2921 - learning_rate: 0.0010\n",
      "Epoch 78/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - accuracy: 0.9826 - loss: 0.0724 - val_accuracy: 0.9356 - val_loss: 0.3097 - learning_rate: 0.0010\n",
      "Epoch 79/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - accuracy: 0.9646 - loss: 0.1399 - val_accuracy: 0.8926 - val_loss: 0.4586 - learning_rate: 0.0010\n",
      "Epoch 80/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 72ms/step - accuracy: 0.9591 - loss: 0.1556 - val_accuracy: 0.8878 - val_loss: 0.5435 - learning_rate: 0.0010\n",
      "Epoch 81/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - accuracy: 0.9406 - loss: 0.2392 - val_accuracy: 0.8568 - val_loss: 0.6319 - learning_rate: 0.0010\n",
      "Epoch 82/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 74ms/step - accuracy: 0.9583 - loss: 0.1374 - val_accuracy: 0.9141 - val_loss: 0.4440 - learning_rate: 0.0010\n",
      "Epoch 83/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - accuracy: 0.9811 - loss: 0.0600 - val_accuracy: 0.9379 - val_loss: 0.2973 - learning_rate: 5.0000e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - accuracy: 0.9893 - loss: 0.0685 - val_accuracy: 0.9379 - val_loss: 0.2904 - learning_rate: 5.0000e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - accuracy: 0.9824 - loss: 0.0712 - val_accuracy: 0.9379 - val_loss: 0.3117 - learning_rate: 5.0000e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - accuracy: 0.9902 - loss: 0.0390 - val_accuracy: 0.9427 - val_loss: 0.3471 - learning_rate: 5.0000e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - accuracy: 0.9894 - loss: 0.0475 - val_accuracy: 0.9356 - val_loss: 0.3206 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0795eecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9475\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30ab1f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('cricket_pose_model.h5')\n",
    "model.save('cricket_pose_model.keras')\n",
    "np.save('cricket_label_map.npy', label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e220a528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "I0000 00:00:1753632962.569709   34338 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1753632962.616315   93594 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 570.172.08), renderer: NVIDIA GeForce RTX 4070 SUPER/PCIe/SSE2\n",
      "W0000 00:00:1753632962.648508   93566 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753632962.666440   93571 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753632962.667323   93575 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753632962.667893   93584 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753632962.668101   93591 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753632962.671462   93573 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753632962.677213   93571 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753632962.680304   93580 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Load trained model and label map\n",
    "model = tf.keras.models.load_model('cricket_pose_model.h5')\n",
    "label_map = np.load('cricket_label_map.npy', allow_pickle=True).item()\n",
    "actions = list(label_map.keys())\n",
    "\n",
    "# Variables for prediction\n",
    "sequence = []\n",
    "sequence_length = 30\n",
    "threshold = 0.7\n",
    "\n",
    "# Start webcam\n",
    "cap = cv.VideoCapture('/home/smayan/Desktop/Cricket Pose Estimation /Model Training/test.mp4')\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Resize for consistent input\n",
    "        frame = cv.resize(frame, (640, 480))\n",
    "\n",
    "        # Detection\n",
    "        image = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = holistic.process(image)\n",
    "        image.flags.writeable = True\n",
    "        image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "\n",
    "        # Draw landmarks\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "            )\n",
    "\n",
    "        # Extract keypoints\n",
    "        if results.pose_landmarks:\n",
    "            keypoints = np.array([[res.x, res.y, res.z, res.visibility]\n",
    "                                  for res in results.pose_landmarks.landmark]).flatten()\n",
    "        else:\n",
    "            keypoints = np.zeros(33*4)\n",
    "\n",
    "        # Append to sequence\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-sequence_length:]\n",
    "\n",
    "        if len(sequence) == sequence_length:\n",
    "            input_seq = np.expand_dims(np.array(sequence), axis=0)\n",
    "            input_seq = input_seq.reshape(1, sequence_length, -1, 1)\n",
    "\n",
    "            # Predict\n",
    "            res = model.predict(input_seq, verbose=0)[0]\n",
    "            predicted_action = actions[np.argmax(res)]\n",
    "            confidence = np.max(res)\n",
    "\n",
    "            # Show prediction\n",
    "            if confidence > threshold:\n",
    "                cv.putText(image, f'{predicted_action}: {confidence:.2f}',\n",
    "                           (10, 50), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            # Show probabilities\n",
    "            for i, (action, prob) in enumerate(zip(actions, res)):\n",
    "                y_pos = 100 + i * 30\n",
    "                cv.rectangle(image, (10, y_pos), (int(prob * 300) + 10, y_pos + 25), (0, 255, 0), -1)\n",
    "                cv.putText(image, f'{action}: {prob:.2f}', (15, y_pos + 18),\n",
    "                           cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "        # Show output\n",
    "        cv.imshow('Cricket Pose Estimation', image)\n",
    "\n",
    "        # Quit\n",
    "        if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47fef18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
